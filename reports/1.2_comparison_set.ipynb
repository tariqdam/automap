{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Distribution Evaluation of AutoMap\n",
    "We are going to evaluate whether we can approve the accuracy of the most probable predicted class by adjusting the predicted probabilities on parameter names based on similarity to already mapped concepts.\n",
    "\n",
    "This means we need to retrieve the original mapped data from disk which is already processed by hand to ensure uniformity and results in a table with at least columns:\n",
    "- `parameter_name`, `concept_label`\n",
    "- `p25`, `p50`, `p75`, `num_records` for numeric data\n",
    "- an entry in `atc` for medication records if available\n",
    "\n",
    "### Concept labels\n",
    "Concept labels are retrieved from the general parameters table. A concept table is also loaded which contains the category structure for the labels for stratified analysis.\n",
    "Output: dictionary concept_groups: {concept_label: category}\n",
    "Output: dictionary concept_label_super_groups {concept_label: concept_label_super}\n",
    "\n",
    "\n",
    "### Reference groups\n",
    "A reference group is a set of previously mapped parameters which describe the expected content of a concept. Instead of defining extreme values for concepts, we retrieve reference values from actual measurements. This is a good idea because extreme values would need to be defined for each concept which is tedious work and up for debate, while using actual measurements may guide in sorting on subtle differences in measured values such as those observed between measured tidal volumes between inspiration and expiration. For each concept's reference group, matching parameters are retrieved and their data is pooled by taking a weighted average of the percentiles based on the number of records. This way, a parameter with a small sample size will have less influence on the overall shape of the data distribution.\n",
    "Note: another approach may be to take the minimum of the p25, the weighted average for p50, and the max of the p75, for parameters with at least N records.\n",
    "Output: nested dictionary of concept labels: {p25: float, p50: float, p75: float, N: int}\n",
    "\n",
    "### Distribution\n",
    "Distribution of underlying data is compared based on the 25th, 50th and 75th percentile, also known as the median and lower and upper quartiles. In an earlier attempt, we used a modified T-statistic to calculate similarity and increase the probability of predicted labels if data distribution was similar. If the T-statisic was low, the increase was higher than when the T-statistic was high, so that concepts with closer reference groups would become more visible. However, this lead to a lot of false positives and tanked the predictions. Therefore, we will now attempt to not select on similarity, but to deselect on dissimilarity by discounting predictions where the reference group is deemed to be dissimilar.\n",
    "Output: Function(parameter_id, predicted_concept_label, parameter_distributions, reference_groups) -> similarity statistic\n",
    "Output: Function(similarity_statistic, probability_of_predicted_concept_label_for_this_parameter) -> probability_of_predicted_concept_label_for_this_parameter || 0\n",
    "Evaluation of performance change compared to base model at major group level\n",
    "\n",
    "### Record type\n",
    "For medication, we can set the probability for non-medication labels to `0` if the parameter contains an `ATC` value. This requires `concept_labels` to contain a grouping structure which labels the concepts as ATC-linked concepts. This can be either achieved through manual labeling of each concept (ideal, but a lot of work), or through the labeling of the concepts based on earlier mapped parameters. For practical purposes, we can assume all concept_labels starting with `med_*` to be medication `concept_labels`. Using the ATC to do a look-up for the concept_label is not preferred, as ATC codes are provided by hospitals and may not be validated as regularly seen during the mapping of the covid project.\n",
    "Output: Function(parameter_id, predicted_concept_label, parameter_distributions) -> parameter_has_atc_and_predicted_concept_is_medication as bool\n",
    "Output: Function(parameter_has_atc_and_predicted_concept_is_medication, probability_of_predicted_concept_label_for_this_parameter) -> probability_of_predicted_concept_label_for_this_parameter || 0\n",
    "Output: Evaluation of performance change compared to base model at major group level\n",
    "Output: Evaluation of performance change compared to base model + distribution at major group level\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "To evaluate the performance of this model, we collect the accuracy, precision, recall and F1-score at concept_label level (tidal_volume_measured_ventilator) without stratifying, as well as accuracy stratified per relevance for relevant/irrelevant records, as well as stratified for major groups (hemodynamic, respiratory, medication), and stratified for minor groups (tidal volume, heart rate) where these are available in the grouping structure. The accuracy is plotted as a cumulative accuracy over the number of predicted labels sorted by their probability, both for the concept_label level and lines for the relevant/irrelevant parameters, over the number of parameters which still receive predicted labels at label position N. The precision, recall and F1-scores are reported as tables with y-values corresponding to the stratified labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple, List, Dict, Union\n",
    "\n",
    "import sklearn\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context(\"paper\", font_scale = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_HOSPITALS = ['vumc', 'amc', 'erasmus', 'olvg']\n",
    "EHR_SYSTEMS = ['epic', 'mv', 'hix']\n",
    "SOURCE = 'parameter_name'\n",
    "TARGET = 'concept_label'\n",
    "HOSPITAL_COLUMN = 'hospital_name'\n",
    "DATA_DISTRIBUTION_COLUMNS = ['amin', 'amax', 'p25', 'p50', 'p75', 'p50_over_iqr', 'iqr_over_p50', 'skewed']\n",
    "DATA_DISTRIBUTION_WEIGHTS = 'num_records'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def calculate_performance(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': sklearn.metrics.accuracy_score(y_true, y_pred),\n",
    "        'precision': sklearn.metrics.precision_score(y_true, y_pred, average='weighted'),\n",
    "        'recall': sklearn.metrics.recall_score(y_true, y_pred, average='weighted'),\n",
    "        'f1_score': sklearn.metrics.f1_score(y_true, y_pred, average='weighted'),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_predictions_to_proportions(predictions: pd.DataFrame,\n",
    "                                         original_data: pd.DataFrame,\n",
    "                                         cumulative_score: bool = False,\n",
    "                                         ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Transform probability predictions from the AutoMap class to proportions of correct predictions per rank stratified over relevance, irrelevance and overall scores to be used for plotting.\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param original_data: pandas DataFrame of the original data as input to the AutoMap class, where each row represents a document and its corresponding true concept labels.\n",
    "    :param cumulative_score: boolean indicating whether to calculate cumulative scores over the top X predictions.\n",
    "    :return: tuple of pandas DataFrames where the first returns the probability while the second returns the number of parameters for which predictions were made\n",
    "    \"\"\"\n",
    "\n",
    "    _predictions = predictions.sort_values(['id', 'value'], ascending=[True, False])\n",
    "    _predictions[TARGET] = _predictions['id'].map(original_data.set_index('id')[TARGET])\n",
    "    _predictions['rank'] = _predictions.groupby('id').cumcount()\n",
    "    _predictions['rank_correct'] = (_predictions['label'] == _predictions[TARGET]).astype(int)\n",
    "    _predictions['relevance'] = (_predictions[TARGET] == 'unmapped').map({True: 'irrelevant',\n",
    "                                                                          False: 'relevant'})\n",
    "    # calculate scores\n",
    "    scores = _predictions.groupby(['relevance', 'rank'])['rank_correct'].sum().reset_index()\n",
    "    scores_plot = scores.pivot(index=['rank'], columns=['relevance'], values=['rank_correct']).fillna(0)\n",
    "    scores_plot.columns = scores_plot.columns.droplevel()\n",
    "    scores_plot['overall'] = scores_plot.sum(axis=1)\n",
    "    if cumulative_score:\n",
    "        scores_plot = scores_plot.cumsum()\n",
    "    scores_plot = scores_plot[sorted(scores_plot.columns)]\n",
    "\n",
    "    # get the number of parameter in each group of relevance\n",
    "    parameter_count = _predictions[['id', 'relevance']].groupby(['relevance'])['id'].nunique()\n",
    "    parameter_count['overall'] = parameter_count.sum()\n",
    "    parameter_count = parameter_count.sort_index()\n",
    "\n",
    "    scores_plot_ratio = scores_plot / parameter_count\n",
    "    print(scores_plot_ratio)\n",
    "\n",
    "    parameter_count = _predictions[['id', 'relevance', 'rank']].groupby(['relevance', 'rank'])['id'].count()\n",
    "    parameter_count = parameter_count.reset_index().pivot(index='rank', columns='relevance', values='id')\n",
    "    parameter_count['overall'] = parameter_count['irrelevant'].fillna(0) + parameter_count['relevant'].fillna(0)\n",
    "    print(parameter_count)\n",
    "\n",
    "    return scores_plot_ratio, parameter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge predictions with original data to retrieve grouping categories\n",
    "def merge_predictions_with_original_data(predictions: pd.DataFrame,\n",
    "                                         original_data: pd.DataFrame,\n",
    "                                         grouping_categories: Dict[str, Dict[str, str]] = None,\n",
    "                                         ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get grouping categories for predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param original_data: pandas DataFrame of the original data as input to the AutoMap class, where each row represents a document and its corresponding true concept labels.\n",
    "    :param grouping_categories: dictionary of column name followed by dictionary to map source to target values\n",
    "    :return: pandas DataFrame with predictions and grouping categories\n",
    "    \"\"\"\n",
    "    _predictions = predictions.sort_values(['id', 'value']).copy()\n",
    "    _predictions = _predictions.merge(original_data, on='id')\n",
    "    if grouping_categories:\n",
    "        for key, values in grouping_categories.items():\n",
    "            _predictions[f\"{key}_groups\"] = _predictions[key].map(values)\n",
    "    return _predictions\n",
    "\n",
    "# assign correct flag to predictions\n",
    "def assign_correct_flag(predictions: pd.DataFrame,\n",
    "                        target_column: str = TARGET,\n",
    "                        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign correct flag to predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param target_column: column name of the target column\n",
    "    :return: pandas DataFrame with predictions and correct flag\n",
    "    \"\"\"\n",
    "    _predictions = predictions.copy()\n",
    "    _predictions['correct'] = _predictions[target_column] == _predictions['label']\n",
    "    return _predictions\n",
    "\n",
    "# assign ranks to predictions\n",
    "def assign_ranks(predictions: pd.DataFrame,\n",
    "                 target_column: str = 'label',\n",
    "                 ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign ranks to predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param target_column: column name of the target column\n",
    "    :return: pandas DataFrame with predictions and ranks\n",
    "    \"\"\"\n",
    "    _predictions = predictions.sort_values(['id', 'value'], ascending=[True, False]).copy()\n",
    "    _predictions['rank'] = _predictions.groupby('id').cumcount() + 1\n",
    "    return _predictions\n",
    "\n",
    "\n",
    "def assign_relevance(predictions: pd.DataFrame,\n",
    "                     target_column: str = TARGET,\n",
    "                     ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign relevance to predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param target_column: column name of the target column\n",
    "    :return: pandas DataFrame with predictions and relevance\n",
    "    \"\"\"\n",
    "    _predictions = predictions.copy()\n",
    "    _predictions['relevance'] = (_predictions[target_column] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    return _predictions\n",
    "\n",
    "def get_processed_data(predictions,\n",
    "                       original_data,\n",
    "                       grouping_categories,\n",
    "                       ):\n",
    "\n",
    "    _predictions = merge_predictions_with_original_data(predictions=predictions,\n",
    "                                                        original_data=original_data,\n",
    "                                                        grouping_categories=grouping_categories)\n",
    "    _predictions = assign_correct_flag(predictions=_predictions)\n",
    "    _predictions = assign_ranks(predictions=_predictions)\n",
    "    _predictions = assign_relevance(predictions=_predictions)\n",
    "    return _predictions\n",
    "\n",
    "\n",
    "def calculate_scores_for_groups(\n",
    "        data: pd.DataFrame,\n",
    "        label_true: str = TARGET,\n",
    "        label_pred: str = 'label',\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate scores for the data passed in.\n",
    "    :param data: pandas DataFrame with at least columns for true labels and predicted labels\n",
    "    :param label_true: string with the name of the column containing the true labels\n",
    "    :param label_pred: string with the name of the column containing the predicted labels\n",
    "    :return: pandas DataFrame with scores for each true label\n",
    "    \"\"\"\n",
    "\n",
    "    result = pd.DataFrame(sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_true=data[label_true],\n",
    "        y_pred=data[label_pred],\n",
    "        labels=data[label_true].unique(),\n",
    "        average=None, #average='weighted',\n",
    "        beta=1,\n",
    "        zero_division=0,\n",
    "    )).transpose().set_index(data[label_true].unique())\n",
    "    result.columns = ['precision', 'recall', 'f1', 'support']\n",
    "    result.index.name = label_true\n",
    "    result = result.reset_index()\n",
    "\n",
    "    result_accuracy = data.groupby([TARGET]).apply(lambda x: sklearn.metrics.accuracy_score(\n",
    "        y_true=x[label_true],\n",
    "        y_pred=x[label_pred],\n",
    "        normalize=True,\n",
    "        sample_weight=None,\n",
    "    )).to_dict()\n",
    "    result['accuracy'] = result[label_true].map(result_accuracy)\n",
    "\n",
    "    result_num_records = data.groupby([TARGET]).apply(lambda x: np.sum(x['num_records'])).to_dict()\n",
    "    result['num_records'] = result[label_true].map(result_num_records)\n",
    "\n",
    "    return result\n",
    "\n",
    "def weighted_average(x: pd.DataFrame,\n",
    "                     score_types: List[str] = None,\n",
    "                     ) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculated the weighted average for each score type if they're present in the dataframe columns. Expects 'support' to contain counts for each score type.\n",
    "    :param x: pandas DataFrame\n",
    "    :param score_types: list of strings with score types to calculate weighted average for\n",
    "    :return: dictionary with weighted average for each score type\n",
    "    \"\"\"\n",
    "    score_types = ['accuracy', 'precision', 'recall', 'f1'] if score_types is None else score_types\n",
    "    sum_cols = ['num_records', 'support', 'group_count']\n",
    "    return_dict = {score_type: np.average(x[score_type], weights=x['support']) for score_type in score_types if score_type in x}\n",
    "    for col in sum_cols:\n",
    "        if col in x:\n",
    "            return_dict[col] = int(np.sum(x[col]))\n",
    "        elif col == 'group_count':\n",
    "            return_dict[col] = len(x)\n",
    "    return return_dict\n",
    "\n",
    "def calculate_scores(predictions: pd.DataFrame,\n",
    "                     original_data: pd.DataFrame,\n",
    "                     grouping_categories: dict = None,\n",
    "                     rank: int = 1,\n",
    "                     ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Retrieves various grouped scores for the publication.\n",
    "    :param predictions: pandas DataFrame of the predictions with at least the columns 'id', 'label', 'value'\n",
    "    :param original_data: panda DataFrame of the original data being predicted on. Must contain the columns 'id', TARGET concept label and ehr_name.\n",
    "    :param grouping_categories: dictionary of column names and a corresponding dictionary to map values to. Groups will be written to {key}_group column.\n",
    "    :param rank: integer of the rank to calculate scores for, default is 1 for the first prediction\n",
    "    :return: dictionary of various grouping structures and the respective table for accuracy/precision/recall/f1-scores and support\n",
    "    \"\"\"\n",
    "    proc = get_processed_data(predictions=predictions,\n",
    "                              original_data=original_data,\n",
    "                              grouping_categories=grouping_categories)\n",
    "    # Table with scores for each concept label --> allows for grouping over data categories and relevance\n",
    "    result = calculate_scores_for_groups(data=proc.loc[proc['rank'] == rank], label_true=TARGET, label_pred='label')\n",
    "    result['concept_label_group'] = result['concept_label'].map(concept_category_groups)\n",
    "    result['relevance'] = (result['concept_label'] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    result = result.loc[result['support'] > 0].copy() # remove concepts that were not available in the test set as they cannot be evaluated\n",
    "\n",
    "    # Table with scores for each concept label group\n",
    "    result_per_concept_label_group = result.groupby(['concept_label_group']).apply(lambda x: weighted_average(x)\n",
    "            ).apply(pd.Series).sort_values('support', ascending=False)\n",
    "\n",
    "    # Table with scores for each relevance group\n",
    "    result_per_relevance_group = result.groupby(['relevance']).apply(lambda x: weighted_average(x)).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    result_prg_overall = weighted_average(result_per_relevance_group.reset_index())\n",
    "    result_prg_overall = pd.DataFrame(result_prg_overall, index=['zoverall'])\n",
    "    result_per_relevance_group = pd.concat([result_prg_overall, result_per_relevance_group]).sort_index(ascending=False)\n",
    "\n",
    "    # Table with scores for each EHR system and Relevance groups --> specifically for table in manuscript\n",
    "    result_ehr = proc.loc[proc['rank'] == rank].groupby(['ehr_name']).apply(lambda x: calculate_scores_for_groups(data=x, label_true=TARGET, label_pred='label'))\n",
    "    result_ehr['relevance'] = (result_ehr['concept_label'] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    result_ehr_prg = result_ehr.reset_index().groupby(['ehr_name', 'relevance']).apply(lambda x: weighted_average(x)).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    # combine relevant and irrelevant into a weighted average overall score\n",
    "    result_ehr_overall = result_ehr_prg.reset_index().groupby(['ehr_name']).apply(\n",
    "        lambda x: weighted_average(x)\n",
    "            ).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    result_ehr_overall = result_ehr_overall.reset_index()\n",
    "    result_ehr_overall['relevance'] = 'zoverall'\n",
    "    result_ehr_overall.set_index(['ehr_name', 'relevance'], inplace=True)\n",
    "    result_ehr_prg = pd.concat([result_ehr_prg, result_ehr_overall]).sort_values(['ehr_name', 'relevance'], ascending=[True, False])\n",
    "\n",
    "    # Table with scores for each EHR, Hospital Name groups, and relevance groups --> specifically for table in manuscript supplementary file\n",
    "    result_ehr_hosp = proc.loc[proc['rank'] == rank].groupby(['ehr_name', 'hospital_name']).apply(lambda x: calculate_scores_for_groups(data=x, label_true=TARGET, label_pred='label'))\n",
    "    result_ehr_hosp['relevance'] = (result_ehr_hosp['concept_label'] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    result_ehr_hosp_prg = result_ehr_hosp.reset_index().groupby(['ehr_name', 'hospital_name', 'relevance']).apply(lambda x: weighted_average(x)).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    # combine relevant and irrelevant into a weighted average overall score\n",
    "    result_ehr_hosp_overall = result_ehr_hosp_prg.reset_index().groupby(['ehr_name', 'hospital_name']).apply(\n",
    "        lambda x: weighted_average(x)\n",
    "            ).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    result_ehr_hosp_overall = result_ehr_hosp_overall.reset_index()\n",
    "    result_ehr_hosp_overall['relevance'] = 'zoverall'\n",
    "    result_ehr_hosp_overall.set_index(['ehr_name', 'hospital_name', 'relevance'], inplace=True)\n",
    "    result_ehr_hosp_prg = pd.concat([result_ehr_hosp_prg, result_ehr_hosp_overall]).sort_values(['ehr_name', 'hospital_name', 'relevance'], ascending=[True, True, False])\n",
    "\n",
    "    return {'label': result,\n",
    "            'label_group': result_per_concept_label_group,\n",
    "            'relevance': result_per_relevance_group,\n",
    "            'ehr_relevance': result_ehr_prg,\n",
    "            'ehr_hosp_relevance': result_ehr_hosp_prg,\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_data(results: Dict[int, Dict[str, pd.DataFrame]],\n",
    "                  dataset: str = 'relevance',\n",
    "                  score_type='recall',\n",
    "                  ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    y_values = pd.concat([results[i][dataset][score_type] for i in range(1, 11)], axis=1)\n",
    "    y_values.columns = list(range(1,11))\n",
    "\n",
    "    s_values = pd.concat([results[i][dataset]['support'] for i in range(1, 11)], axis=1)\n",
    "    s_values.columns = list(range(1,11))\n",
    "    s_values = s_values.astype(int)\n",
    "    return y_values.transpose(), s_values.transpose()\n",
    "\n",
    "def plot_results(results: Dict[int, Dict[str, pd.DataFrame]],\n",
    "                 dataset: str = 'relevance',\n",
    "                 score_type: str = 'recall',\n",
    "                 N: int = 10,\n",
    "                 cumulative: bool = True,\n",
    "                 plot_order=None,\n",
    "                 color_palette=None,\n",
    "                 save_loc=None,\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Plots the results of the evaluation.\n",
    "    :param results:\n",
    "    :param score_type:\n",
    "    :param N:\n",
    "    :param cumulative:\n",
    "    :param plot_order:\n",
    "    :param color_palette:\n",
    "    :param save_loc:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    score_data, count_data = get_plot_data(results=results, dataset=dataset, score_type=score_type)\n",
    "\n",
    "    if cumulative:\n",
    "        score_data = score_data.cumsum()\n",
    "    score_data.to_csv(f'{save_loc}score_data__{dataset}__{score_type}.csv')\n",
    "    count_data.to_csv(f'{save_loc}count_data__{dataset}__{score_type}.csv')\n",
    "    plot_order = sorted(score_data.columns, reverse=True) if plot_order is None else plot_order\n",
    "    plot_order_rename_dict = {x: x.replace('zoverall', 'overall').capitalize() for x in plot_order}\n",
    "    score_data.rename(columns=plot_order_rename_dict, inplace=True)\n",
    "    count_data.rename(columns=plot_order_rename_dict, inplace=True)\n",
    "    plot_order = plot_order_rename_dict.values()\n",
    "    c_palette = ['black'] * len(plot_order) if color_palette is None else color_palette[0:len(plot_order)] #['black'] * count_values.shape[1]\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1,\n",
    "                                   sharex=True,\n",
    "                                   figsize=(6,6),\n",
    "                                   gridspec_kw={'height_ratios': [6,2],\n",
    "                                                },\n",
    "                                   )\n",
    "\n",
    "    # Plot scores\n",
    "    sns.lineplot(data=score_data[plot_order],\n",
    "                 palette=c_palette,\n",
    "                 legend=True,\n",
    "                 ax=ax1,\n",
    "                 )\n",
    "    ax1.set_xlabel('Rank of predicted labels')\n",
    "    ax1.set_ylabel(f\"{score_type}\".capitalize(), labelpad=25)\n",
    "    ax1.set_ylim(0, 1.01)\n",
    "    ax1.set_xlim(1, N)\n",
    "    ax1.legend(\n",
    "        loc='lower right',\n",
    "        bbox_to_anchor=(1.0, 0.0),\n",
    "        ncol=1,\n",
    "    )\n",
    "\n",
    "    # Plot parameter counts\n",
    "    sns.lineplot(data=count_data[plot_order],\n",
    "                 palette=c_palette,\n",
    "                 legend=False,\n",
    "                 ax=ax2,\n",
    "                 )\n",
    "    ax2.set_ylim(0, count_data.max().max()*1.05)\n",
    "    ax2.set_xlim(1, N)\n",
    "    ax2.set_xticks(list(range(1, N+1)))\n",
    "    ax2.set_xticklabels(list(range(1, N+1)))\n",
    "    ax2.set_xlabel('Number of predicted labels')\n",
    "    ax2.set_ylabel('Parameter\\ncount')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_loc}plot__{dataset}__{score_type}.png\", dpi=1200)\n",
    "    plt.savefig(f\"{save_loc}plot__{dataset}__{score_type}.pdf\", dpi=1200)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get concept groupings\n",
    "\n",
    "Concept groups are dictionaries which translate individual concept labels to their respective groups to enable reporting on performance stratified over major categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_concept_grouping(data, source, target) -> dict:\n",
    "    assert data[source].duplicated().sum() == 0, 'source column is not unique'\n",
    "    return data.set_index(source)[target].to_dict()\n",
    "\n",
    "concepts = pd.read_csv('../data/input/concepts.csv')\n",
    "concept_category_groups = create_concept_grouping(concepts, 'concept_label', 'category')\n",
    "concept_label_super_groups = create_concept_grouping(concepts, 'concept_label', 'concept_label_super')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get reference groups\n",
    "\n",
    "Reference groups are concept labels which use the training hospital's mapped parameters to retrieve an average value for their underlying data content. Reference groups contain averages for p25, p50, p75, weighted by the number of records per parameter, as well as normalized values for their distribution, such as p50_over_iqr and iqr_over_p50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/input/static/train_set.csv')\n",
    "print(df_train.shape)\n",
    "df_test = pd.read_csv('../data/input/static/test_set.csv')\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train:', df_train['hospital_name'].nunique(), df_train['hospital_name'].unique())\n",
    "\n",
    "print('Test:', df_test['hospital_name'].nunique(), df_test['hospital_name'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['id'] = df_train.reset_index(drop=True).index\n",
    "df_test['id'] = df_test.reset_index(drop=True).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_groups(data: pd.DataFrame,\n",
    "                            group_by: Union[List[str], str],\n",
    "                            filter_by: List[str],\n",
    "                            filter_on: List[str],\n",
    "                            values: List[str],\n",
    "                            weights: str,\n",
    "                            ) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Create reference groups for a given set of parameters.\n",
    "    :param data:\n",
    "    :param group_by:\n",
    "    :param filter_by:\n",
    "    :param filter_on:\n",
    "    :param values:\n",
    "    :param weights:\n",
    "    :return: dictionary with keys corresponding to concept labels\n",
    "    \"\"\"\n",
    "    var = data.loc[data[filter_by].isin(filter_on)].groupby(group_by, group_keys=False)\n",
    "    d = dict()\n",
    "    for value in values:\n",
    "        if value == 'amin':\n",
    "            d[value] = var[value].min().to_dict()\n",
    "        elif value == 'amax':\n",
    "            d[value] = var[value].max().to_dict()\n",
    "        else:\n",
    "            d[value] = var.apply(lambda x: calculate_average_weight(x=x, value=value, weights=weights)).to_dict()\n",
    "    if 'p50' in values and 'p25' in values and 'p75' in values:\n",
    "        d['p75_max'] = var['p75'].max().to_dict()\n",
    "        d['p25_min'] = var['p25'].min().to_dict()\n",
    "    d[weights] = var.apply(lambda x: x[weights].sum()).to_dict()\n",
    "    return pd.DataFrame(d).transpose().to_dict()\n",
    "\n",
    "def calculate_average_weight(x: pd.DataFrame,\n",
    "                             value: str,\n",
    "                             weights: str) -> float:\n",
    "    return np.average(x[value], weights=x[weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get base predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "AutoMap class\n",
    "\"\"\"\n",
    "\n",
    "# import\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "class AutoMap:\n",
    "    \"\"\"\n",
    "    :params:\n",
    "    train_files: list of files to load for training the prediction pipeline on\n",
    "    predict_files: list of files to read and predict the parameters for\n",
    "    output_files: list of files to save the predictions to, where length equals length of predict_files\n",
    "    pipe_file: Previously created compatible pickled tuple of fitted (Pipeline, LabelEncoder) objects\n",
    "\n",
    "\n",
    "    :args:\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')  # hidden in Pipe creation\n",
    "\n",
    "        # column names to use in training/predicting\n",
    "        self.source = 'parameter_name'\n",
    "        self.target = 'pacmed_subname'\n",
    "        self.pred = 'predicted_subname'\n",
    "\n",
    "        # if no label is given, impute with index[0] (unmapped)\n",
    "        self.unlabeled = ['unmapped', 'microbiology']  # used for validation to filter out unlabeled\n",
    "\n",
    "        # initial r'\\w+' but 5% performance gain when underscores are omitted\n",
    "        self.preprocess_text_regex_expression = r'[a-zA-Z0-9]+'\n",
    "        return\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Tokenise words while ignoring punctuation\n",
    "        tokeniser = RegexpTokenizer(self.preprocess_text_regex_expression)\n",
    "        tokens = tokeniser.tokenize(text)\n",
    "\n",
    "        # Lowercase and lemmatise\n",
    "        lemmatiser = WordNetLemmatizer()\n",
    "        lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "\n",
    "        # Remove stop words\n",
    "        # keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "        # return keywords\n",
    "        return lemmas\n",
    "\n",
    "    def create_pipe(self,\n",
    "                    X=pd.Series,\n",
    "                    y=pd.Series,\n",
    "                    estimator=SGDClassifier(random_state=123),\n",
    "                    grid: dict = None,\n",
    "                    cv: int = 10,\n",
    "                    n_jobs: int = None,\n",
    "                    save: bool = False,\n",
    "                    prefix=None):\n",
    "        \"\"\"\n",
    "        Create the pipe object used to train and test text data\n",
    "        \"\"\"\n",
    "\n",
    "        if y.isna().sum() > 0:\n",
    "            y = y.fillna(self.unlabeled[0])\n",
    "\n",
    "        # ensure labels are encoded\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(y=y.unique())\n",
    "\n",
    "        # Create an instance of TfidfVectorizer\n",
    "        vectoriser = TfidfVectorizer(analyzer=self.preprocess_text)\n",
    "\n",
    "        # Fit to the data and transform to feature matrix\n",
    "        X_train_tfidf = vectoriser.fit_transform(X)\n",
    "\n",
    "        # try an initial accuracy before hyperparameter optimization\n",
    "        clf = estimator\n",
    "        # clf = SGDClassifier(random_state=123)\n",
    "        # clf_scores = cross_val_score(clf, X_train_tfidf, self.y_train, cv=10)\n",
    "        # print(clf_scores)\n",
    "        # print(\"SGDClassfier Accuracy: %0.2f (+/- %0.2f)\" % (clf_scores.mean(), clf_scores.std() * 2))\n",
    "\n",
    "        if grid is None:\n",
    "            grid = {'fit_intercept': [True, False],\n",
    "                    'early_stopping': [True, False],\n",
    "                    'loss': ['log', 'modified_huber', 'perceptron', 'huber', 'squared_loss', 'epsilon_insensitive',\n",
    "                             'squared_epsilon_insensitive'],\n",
    "                    # ['hinge', 'log', 'squared_hinge'], #PM squared_loss --> squared_error in v1.2\n",
    "                    'penalty': ['l2', 'l1', 'none']}\n",
    "\n",
    "            # Reduce to optimal grid for rerunning code\n",
    "            grid = {'fit_intercept': [True],\n",
    "                    'early_stopping': [False],\n",
    "                    'loss': ['modified_huber'],\n",
    "                    'penalty': ['l2']}\n",
    "\n",
    "        # retry the SGDClassifier training with param_grid\n",
    "        search = GridSearchCV(estimator=clf, param_grid=grid, cv=cv, n_jobs=n_jobs)\n",
    "        search.fit(X_train_tfidf, y)\n",
    "\n",
    "        # grid_sgd_clf_scores = cross_val_score(search.best_estimator_, X_train_tfidf, self.y_train, cv=5)\n",
    "        # print(grid_sgd_clf_scores)\n",
    "        # print(\"SGDClassifier optimal grid Accuracy: %0.2f (+/- %0.2f)\" % (\n",
    "        # grid_sgd_clf_scores.mean(), grid_sgd_clf_scores.std() * 2))\n",
    "\n",
    "        # create Pipeline with vectoriser and optimal classifier\n",
    "        self.pipe = Pipeline([('vectoriser', vectoriser),\n",
    "                              ('classifier', search)])  # clf\n",
    "\n",
    "        # fit the pipeline to the full training data\n",
    "        self.pipe.fit(X, self.le.transform(y.values))\n",
    "\n",
    "        # save pipe to file to prevent rerunning the same pipelines\n",
    "        if prefix is None:\n",
    "            prefix = ''\n",
    "        if save:\n",
    "            f_name = f'./data/pipes/{prefix}__{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.pipe'\n",
    "            joblib.dump((self.pipe,\n",
    "                         self.le,\n",
    "                         ),\n",
    "                        f_name,\n",
    "                        compress=('gzip', 3),\n",
    "                        protocol=5)\n",
    "            print(f\"Pipeline saved to: {f_name}\")\n",
    "\n",
    "        return self.pipe\n",
    "\n",
    "    def save_pipe(self, f_name):\n",
    "        joblib.dump((self.pipe, self.le), f_name)\n",
    "        print(f\"Pipeline saved to: {f_name}\")\n",
    "\n",
    "    def load_pipe(self, f_name):\n",
    "        if os.path.isfile(f_name):\n",
    "            self.pipe, self.le = joblib.load(f_name)\n",
    "        else:\n",
    "            self.pipe = None\n",
    "            self.le = LabelEncoder()\n",
    "        print(f\"Pipeline loaded from: {f_name}\")\n",
    "\n",
    "    def predict_proba_transformed(self, X, **predict_proba_params):\n",
    "\n",
    "        if isinstance(X, pd.Series):\n",
    "            probs = self.pipe.predict_proba(X, **predict_proba_params)\n",
    "            id_vars = [X.name]\n",
    "            X = pd.DataFrame(X)\n",
    "        else:\n",
    "            probs = self.pipe.predict_proba(X[X.columns[1]], **predict_proba_params)\n",
    "            id_vars = list(X.columns)\n",
    "            print(id_vars)\n",
    "\n",
    "        c = pd.concat(\n",
    "            [X.reset_index(drop=True),\n",
    "             pd.DataFrame(probs, columns=self.le.classes_),\n",
    "             ],\n",
    "            axis=1)\n",
    "        c.loc[:, self.le.classes_] = c.loc[:, self.le.classes_].replace(0, np.nan)\n",
    "        return (c\n",
    "            .set_index(id_vars)\n",
    "            .stack()\n",
    "            .reset_index()\n",
    "            .rename(columns={\n",
    "                \"level_1\": \"label\",\n",
    "                \"level_2\": \"label\",\n",
    "                0: \"value\",\n",
    "            }\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./output/static/overlapping', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am = AutoMap()\n",
    "\n",
    "train_data = df_train.copy()\n",
    "test_data = df_test.copy()\n",
    "\n",
    "am.create_pipe(X=train_data[SOURCE],\n",
    "               y=train_data[TARGET],\n",
    "               cv=2, n_jobs=-1,\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = am.pipe.predict_proba(test_data[SOURCE])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.le.classes_\n",
    "y_true = am.le.transform(test_data[TARGET])\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score\n",
    "top_k_accuracy_score(y_true=y_true,\n",
    "                     y_score=predictions,\n",
    "                     k=1,\n",
    "                     labels=range(0, len(am.le.classes_)),\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score\n",
    "top_k_accuracy_score(y_true=y_true,\n",
    "                     y_score=predictions,\n",
    "                     k=5,\n",
    "                     labels=range(0, len(am.le.classes_)),\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = am.predict_proba_transformed(test_data[['id', SOURCE]]).sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels[TARGET + '_original'] = predicted_labels.id.map(test_data.set_index('id')[TARGET].to_dict())\n",
    "\n",
    "save_loc='./output/static/overlapping/'\n",
    "\n",
    "predicted_labels.to_csv(f'{save_loc}predicted_labels.csv')\n",
    "test_data.to_csv(f'{save_loc}test_data.csv')\n",
    "train_data.to_csv(f'{save_loc}train_data.csv')\n",
    "joblib.dump(am, f'{save_loc}am.joblib')\n",
    "\n",
    "results = dict()\n",
    "for i in range(1, 11):\n",
    "    results[i] = calculate_scores(predicted_labels, test_data[['id', 'hospital_name' , 'ehr_name', 'concept_label', 'num_records']], {TARGET: concept_category_groups}, rank=i)\n",
    "    for key, value in results[i].items():\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            for col in value.columns:\n",
    "                if value[col].dtype == 'float64':\n",
    "                    if (value[col] == value[col].astype(int).astype(float)).all():\n",
    "                        value[col] = value[col].astype(int)\n",
    "            value.to_csv(f'{save_loc}aprf__rank_{i}__{key}.csv')\n",
    "            value.round(3).to_csv(f'{save_loc}aprf__rank_{i}__{key}__round3.csv', float_format='%.3f')\n",
    "            if (key == 'relevance') & (i == 1):\n",
    "                print(value.round(3))\n",
    "\n",
    "plot_results(results, score_type='precision', save_loc=save_loc)\n",
    "plot_results(results, score_type='recall', save_loc=save_loc)\n",
    "plot_results(results, score_type='f1', save_loc=save_loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
