{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-One-Out Cross Validation\n",
    "Train on all hospitals but one and test on the left-out hospital. Repeat for each hospital. This is a good way to test the model's generalizability to new hospitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from typing import Tuple, List, Dict, Union\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "AutoMap class\n",
    "\"\"\"\n",
    "\n",
    "# import\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "class AutoMap:\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')  # hidden in Pipe creation\n",
    "\n",
    "        # column names to use in training/predicting\n",
    "        self.source = 'parameter_name'\n",
    "        self.target = 'pacmed_subname'\n",
    "        self.pred = 'predicted_subname'\n",
    "\n",
    "        # if no label is given, impute with index[0] (unmapped)\n",
    "        self.unlabeled = ['unmapped', 'microbiology']  # used for validation to filter out unlabeled\n",
    "\n",
    "        # initial r'\\w+' but 5% performance gain when underscores are omitted\n",
    "        self.preprocess_text_regex_expression = r'[a-zA-Z0-9]+'\n",
    "        return\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Tokenise words while ignoring punctuation\n",
    "        tokeniser = RegexpTokenizer(self.preprocess_text_regex_expression)\n",
    "        tokens = tokeniser.tokenize(text)\n",
    "\n",
    "        # Lowercase and lemmatise\n",
    "        lemmatiser = WordNetLemmatizer()\n",
    "        lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "\n",
    "        # Remove stop words\n",
    "        # keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "        # return keywords\n",
    "        return lemmas\n",
    "\n",
    "    def create_pipe(self,\n",
    "                    X=pd.Series,\n",
    "                    y=pd.Series,\n",
    "                    estimator=SGDClassifier(random_state=123),\n",
    "                    grid: dict = None,\n",
    "                    cv: int = 10,\n",
    "                    n_jobs: int = None,\n",
    "                    save: bool = False,\n",
    "                    prefix=None):\n",
    "        \"\"\"\n",
    "        Create the pipe object used to train and test text data\n",
    "        \"\"\"\n",
    "\n",
    "        if y.isna().sum() > 0:\n",
    "            y = y.fillna(self.unlabeled[0])\n",
    "\n",
    "        # ensure labels are encoded\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(y=y.unique())\n",
    "\n",
    "        # Create an instance of TfidfVectorizer\n",
    "        vectoriser = TfidfVectorizer(analyzer=self.preprocess_text)\n",
    "\n",
    "        # Fit to the data and transform to feature matrix\n",
    "        X_train_tfidf = vectoriser.fit_transform(X)\n",
    "\n",
    "        # try an initial accuracy before hyperparameter optimization\n",
    "        clf = estimator\n",
    "        # clf = SGDClassifier(random_state=123)\n",
    "        # clf_scores = cross_val_score(clf, X_train_tfidf, self.y_train, cv=10)\n",
    "        # print(clf_scores)\n",
    "        # print(\"SGDClassfier Accuracy: %0.2f (+/- %0.2f)\" % (clf_scores.mean(), clf_scores.std() * 2))\n",
    "\n",
    "        if grid is None:\n",
    "            grid = {'fit_intercept': [True, False],\n",
    "                    'early_stopping': [True, False],\n",
    "                    'loss': ['log', 'modified_huber', 'perceptron', 'huber', 'squared_loss', 'epsilon_insensitive',\n",
    "                             'squared_epsilon_insensitive'],\n",
    "                    # ['hinge', 'log', 'squared_hinge'], #PM squared_loss --> squared_error in v1.2\n",
    "                    'penalty': ['l2', 'l1', 'none']}\n",
    "\n",
    "            # Reduce to optimal grid for rerunning code\n",
    "            grid = {'fit_intercept': [True],\n",
    "                    'early_stopping': [False],\n",
    "                    'loss': ['modified_huber'],\n",
    "                    'penalty': ['elasticnet']}\n",
    "\n",
    "        # retry the SGDClassifier training with param_grid\n",
    "        search = GridSearchCV(estimator=clf, param_grid=grid, cv=cv, n_jobs=n_jobs)\n",
    "        search.fit(X_train_tfidf, y)\n",
    "\n",
    "        # grid_sgd_clf_scores = cross_val_score(search.best_estimator_, X_train_tfidf, self.y_train, cv=5)\n",
    "        # print(grid_sgd_clf_scores)\n",
    "        # print(\"SGDClassifier optimal grid Accuracy: %0.2f (+/- %0.2f)\" % (\n",
    "        # grid_sgd_clf_scores.mean(), grid_sgd_clf_scores.std() * 2))\n",
    "\n",
    "        # create Pipeline with vectoriser and optimal classifier\n",
    "        self.pipe = Pipeline([('vectoriser', vectoriser),\n",
    "                              ('classifier', search)])  # clf\n",
    "\n",
    "        # fit the pipeline to the full training data\n",
    "        self.pipe.fit(X, self.le.transform(y.values))\n",
    "\n",
    "        # save pipe to file to prevent rerunning the same pipelines\n",
    "        if prefix is None:\n",
    "            prefix = ''\n",
    "        if save:\n",
    "            f_name = f'./data/pipes/{prefix}__{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.pipe'\n",
    "            joblib.dump((self.pipe,\n",
    "                         self.le,\n",
    "                         ),\n",
    "                        f_name,\n",
    "                        compress=('gzip', 3),\n",
    "                        protocol=5)\n",
    "            print(f\"Pipeline saved to: {f_name}\")\n",
    "\n",
    "        return self.pipe\n",
    "\n",
    "    def save_pipe(self, f_name):\n",
    "        joblib.dump((self.pipe, self.le), f_name)\n",
    "        print(f\"Pipeline saved to: {f_name}\")\n",
    "\n",
    "    def load_pipe(self, f_name):\n",
    "        if os.path.isfile(f_name):\n",
    "            self.pipe, self.le = joblib.load(f_name)\n",
    "        else:\n",
    "            self.pipe = None\n",
    "            self.le = LabelEncoder()\n",
    "        print(f\"Pipeline loaded from: {f_name}\")\n",
    "\n",
    "    def predict_proba_transformed(self, X, **predict_proba_params):\n",
    "\n",
    "        if isinstance(X, pd.Series):\n",
    "            probs = self.pipe.predict_proba(X, **predict_proba_params)\n",
    "            id_vars = [X.name]\n",
    "            X = pd.DataFrame(X)\n",
    "        else:\n",
    "            probs = self.pipe.predict_proba(X[X.columns[1]], **predict_proba_params)\n",
    "            id_vars = list(X.columns)\n",
    "            print(id_vars)\n",
    "\n",
    "        c = pd.concat(\n",
    "            [X.reset_index(drop=True),\n",
    "             pd.DataFrame(probs, columns=self.le.classes_),\n",
    "             ],\n",
    "            axis=1)\n",
    "        c.loc[:, self.le.classes_] = c.loc[:, self.le.classes_].replace(0, np.nan)\n",
    "        return (c\n",
    "            .set_index(id_vars)\n",
    "            .stack()\n",
    "            .reset_index()\n",
    "            .rename(columns={\n",
    "                \"level_1\": \"label\",\n",
    "                \"level_2\": \"label\",\n",
    "                0: \"value\",\n",
    "            }\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_HOSPITALS = ['vumc', 'amc', 'erasmus', 'olvg']\n",
    "EHR_SYSTEMS = ['epic', 'mv', 'hix']\n",
    "SOURCE = 'parameter_name'\n",
    "TARGET = 'concept_label'\n",
    "HOSPITAL_COLUMN = 'hospital_name'\n",
    "DATA_DISTRIBUTION_COLUMNS = ['amin', 'amax', 'p25', 'p50', 'p75', 'p50_over_iqr', 'iqr_over_p50', 'skewed']\n",
    "DATA_DISTRIBUTION_WEIGHTS = 'num_records'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_predictions_to_proportions(predictions: pd.DataFrame,\n",
    "                                         original_data: pd.DataFrame,\n",
    "                                         cumulative_score: bool = False,\n",
    "                                         ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Transform probability predictions from the AutoMap class to proportions of correct predictions per rank stratified over relevance, irrelevance and overall scores to be used for plotting.\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param original_data: pandas DataFrame of the original data as input to the AutoMap class, where each row represents a document and its corresponding true concept labels.\n",
    "    :param cumulative_score: boolean indicating whether to calculate cumulative scores over the top X predictions.\n",
    "    :return: tuple of pandas DataFrames where the first returns the probability while the second returns the number of parameters for which predictions were made\n",
    "    \"\"\"\n",
    "\n",
    "    _predictions = predictions.sort_values(['id', 'value'], ascending=[True, False])\n",
    "    _predictions[TARGET] = _predictions['id'].map(original_data.set_index('id')[TARGET])\n",
    "    _predictions['rank'] = _predictions.groupby('id').cumcount()\n",
    "    _predictions['rank_correct'] = (_predictions['label'] == _predictions[TARGET]).astype(int)\n",
    "    _predictions['relevance'] = (_predictions[TARGET] == 'unmapped').map({True: 'irrelevant',\n",
    "                                                                          False: 'relevant'})\n",
    "    # calculate scores\n",
    "    scores = _predictions.groupby(['relevance', 'rank'])['rank_correct'].sum().reset_index()\n",
    "    scores_plot = scores.pivot(index=['rank'], columns=['relevance'], values=['rank_correct']).fillna(0)\n",
    "    scores_plot.columns = scores_plot.columns.droplevel()\n",
    "    scores_plot['overall'] = scores_plot.sum(axis=1)\n",
    "    if cumulative_score:\n",
    "        scores_plot = scores_plot.cumsum()\n",
    "    scores_plot = scores_plot[sorted(scores_plot.columns)]\n",
    "\n",
    "    # get the number of parameter in each group of relevance\n",
    "    parameter_count = _predictions[['id', 'relevance']].groupby(['relevance'])['id'].nunique()\n",
    "    parameter_count['overall'] = parameter_count.sum()\n",
    "    parameter_count = parameter_count.sort_index()\n",
    "\n",
    "    scores_plot_ratio = scores_plot / parameter_count\n",
    "    print(scores_plot_ratio)\n",
    "\n",
    "    parameter_count = _predictions[['id', 'relevance', 'rank']].groupby(['relevance', 'rank'])['id'].count()\n",
    "    parameter_count = parameter_count.reset_index().pivot(index='rank', columns='relevance', values='id')\n",
    "    parameter_count['overall'] = parameter_count['irrelevant'].fillna(0) + parameter_count['relevant'].fillna(0)\n",
    "    print(parameter_count)\n",
    "\n",
    "    return scores_plot_ratio, parameter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge predictions with original data to retrieve grouping categories\n",
    "def merge_predictions_with_original_data(predictions: pd.DataFrame,\n",
    "                                         original_data: pd.DataFrame,\n",
    "                                         grouping_categories: Dict[str, Dict[str, str]] = None,\n",
    "                                         ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get grouping categories for predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param original_data: pandas DataFrame of the original data as input to the AutoMap class, where each row represents a document and its corresponding true concept labels.\n",
    "    :param grouping_categories: dictionary of column name followed by dictionary to map source to target values\n",
    "    :return: pandas DataFrame with predictions and grouping categories\n",
    "    \"\"\"\n",
    "    _predictions = predictions.sort_values(['id', 'value']).copy()\n",
    "    _predictions = _predictions.merge(original_data, on='id')\n",
    "    if grouping_categories:\n",
    "        for key, values in grouping_categories.items():\n",
    "            _predictions[f\"{key}_groups\"] = _predictions[key].map(values)\n",
    "    return _predictions\n",
    "\n",
    "# assign correct flag to predictions\n",
    "def assign_correct_flag(predictions: pd.DataFrame,\n",
    "                        target_column: str = TARGET,\n",
    "                        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign correct flag to predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param target_column: column name of the target column\n",
    "    :return: pandas DataFrame with predictions and correct flag\n",
    "    \"\"\"\n",
    "    _predictions = predictions.copy()\n",
    "    _predictions['correct'] = _predictions[target_column] == _predictions['label']\n",
    "    return _predictions\n",
    "\n",
    "# assign ranks to predictions\n",
    "def assign_ranks(predictions: pd.DataFrame,\n",
    "                 target_column: str = 'label',\n",
    "                 ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign ranks to predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param target_column: column name of the target column\n",
    "    :return: pandas DataFrame with predictions and ranks\n",
    "    \"\"\"\n",
    "    _predictions = predictions.sort_values(['id', 'value'], ascending=[True, False]).copy()\n",
    "    _predictions['rank'] = _predictions.groupby('id').cumcount() + 1\n",
    "    return _predictions\n",
    "\n",
    "\n",
    "def assign_relevance(predictions: pd.DataFrame,\n",
    "                     target_column: str = TARGET,\n",
    "                     ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign relevance to predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param target_column: column name of the target column\n",
    "    :return: pandas DataFrame with predictions and relevance\n",
    "    \"\"\"\n",
    "    _predictions = predictions.copy()\n",
    "    _predictions['relevance'] = (_predictions[target_column] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    return _predictions\n",
    "\n",
    "def get_processed_data(predictions,\n",
    "                       original_data,\n",
    "                       grouping_categories,\n",
    "                       ):\n",
    "\n",
    "    _predictions = merge_predictions_with_original_data(predictions=predictions,\n",
    "                                                        original_data=original_data,\n",
    "                                                        grouping_categories=grouping_categories)\n",
    "    _predictions = assign_correct_flag(predictions=_predictions)\n",
    "    _predictions = assign_ranks(predictions=_predictions)\n",
    "    _predictions = assign_relevance(predictions=_predictions)\n",
    "    return _predictions\n",
    "\n",
    "\n",
    "def calculate_scores_for_groups(\n",
    "        data: pd.DataFrame,\n",
    "        label_true: str = TARGET,\n",
    "        label_pred: str = 'label',\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate scores for the data passed in.\n",
    "    :param data: pandas DataFrame with at least columns for true labels and predicted labels\n",
    "    :param label_true: string with the name of the column containing the true labels\n",
    "    :param label_pred: string with the name of the column containing the predicted labels\n",
    "    :return: pandas DataFrame with scores for each true label\n",
    "    \"\"\"\n",
    "\n",
    "    result = pd.DataFrame(sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_true=data[label_true],\n",
    "        y_pred=data[label_pred],\n",
    "        labels=data[label_true].unique(),\n",
    "        average=None, #average='weighted',\n",
    "        beta=1,\n",
    "        zero_division=0,\n",
    "    )).transpose().set_index(data[label_true].unique())\n",
    "    result.columns = ['precision', 'recall', 'f1', 'support']\n",
    "    result.index.name = label_true\n",
    "    result = result.reset_index()\n",
    "\n",
    "    result_accuracy = data.groupby([TARGET]).apply(lambda x: sklearn.metrics.accuracy_score(\n",
    "        y_true=x[label_true],\n",
    "        y_pred=x[label_pred],\n",
    "        normalize=True,\n",
    "        sample_weight=None,\n",
    "    )).to_dict()\n",
    "    result['accuracy'] = result[label_true].map(result_accuracy)\n",
    "\n",
    "    result_num_records = data.groupby([TARGET]).apply(lambda x: np.sum(x['num_records'])).to_dict()\n",
    "    result['num_records'] = result[label_true].map(result_num_records)\n",
    "\n",
    "    return result\n",
    "\n",
    "def weighted_average(x: pd.DataFrame,\n",
    "                     score_types: List[str] = None,\n",
    "                     ) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculated the weighted average for each score type if they're present in the dataframe columns. Expects 'support' to contain counts for each score type.\n",
    "    :param x: pandas DataFrame\n",
    "    :param score_types: list of strings with score types to calculate weighted average for\n",
    "    :return: dictionary with weighted average for each score type\n",
    "    \"\"\"\n",
    "    score_types = ['accuracy', 'precision', 'recall', 'f1'] if score_types is None else score_types\n",
    "    sum_cols = ['num_records', 'support', 'group_count']\n",
    "    return_dict = {score_type: np.average(x[score_type], weights=x['support']) for score_type in score_types if score_type in x}\n",
    "    for col in sum_cols:\n",
    "        if col in x:\n",
    "            return_dict[col] = int(np.sum(x[col]))\n",
    "        elif col == 'group_count':\n",
    "            return_dict[col] = len(x)\n",
    "    return return_dict\n",
    "\n",
    "def calculate_scores(predictions: pd.DataFrame,\n",
    "                     original_data: pd.DataFrame,\n",
    "                     grouping_categories: dict = None,\n",
    "                     rank: int = 1,\n",
    "                     ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Retrieves various grouped scores for the publication.\n",
    "    :param predictions: pandas DataFrame of the predictions with at least the columns 'id', 'label', 'value'\n",
    "    :param original_data: panda DataFrame of the original data being predicted on. Must contain the columns 'id', TARGET concept label and ehr_name.\n",
    "    :param grouping_categories: dictionary of column names and a corresponding dictionary to map values to. Groups will be written to {key}_group column.\n",
    "    :param rank: integer of the rank to calculate scores for, default is 1 for the first prediction\n",
    "    :return: dictionary of various grouping structures and the respective table for accuracy/precision/recall/f1-scores and support\n",
    "    \"\"\"\n",
    "    proc = get_processed_data(predictions=predictions,\n",
    "                              original_data=original_data,\n",
    "                              grouping_categories=grouping_categories)\n",
    "    # Table with scores for each concept label --> allows for grouping over data categories and relevance\n",
    "    result = calculate_scores_for_groups(data=proc.loc[proc['rank'] == rank], label_true=TARGET, label_pred='label')\n",
    "    result['concept_label_group'] = result['concept_label'].map(concept_category_groups)\n",
    "    result['relevance'] = (result['concept_label'] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    result = result.loc[result['support'] > 0].copy() # remove concepts that were not available in the test set as they cannot be evaluated\n",
    "\n",
    "    # Table with scores for each concept label group\n",
    "    result_per_concept_label_group = result.groupby(['concept_label_group']).apply(lambda x: weighted_average(x)\n",
    "            ).apply(pd.Series).sort_values('support', ascending=False)\n",
    "\n",
    "    # Table with scores for each relevance group\n",
    "    result_per_relevance_group = result.groupby(['relevance']).apply(lambda x: weighted_average(x)).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    result_prg_overall = weighted_average(result_per_relevance_group.reset_index())\n",
    "    result_prg_overall = pd.DataFrame(result_prg_overall, index=['zoverall'])\n",
    "    result_per_relevance_group = pd.concat([result_prg_overall, result_per_relevance_group]).sort_index(ascending=False)\n",
    "\n",
    "    # Table with scores for each EHR system and Relevance groups --> specifically for table in manuscript\n",
    "    result_ehr = proc.loc[proc['rank'] == rank].groupby(['ehr_name']).apply(lambda x: calculate_scores_for_groups(data=x, label_true=TARGET, label_pred='label'))\n",
    "    result_ehr['relevance'] = (result_ehr['concept_label'] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    result_ehr_prg = result_ehr.reset_index().groupby(['ehr_name', 'relevance']).apply(lambda x: weighted_average(x)).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    # combine relevant and irrelevant into a weighted average overall score\n",
    "    result_ehr_overall = result_ehr_prg.reset_index().groupby(['ehr_name']).apply(\n",
    "        lambda x: weighted_average(x)\n",
    "            ).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    result_ehr_overall = result_ehr_overall.reset_index()\n",
    "    result_ehr_overall['relevance'] = 'zoverall'\n",
    "    result_ehr_overall.set_index(['ehr_name', 'relevance'], inplace=True)\n",
    "    result_ehr_prg = pd.concat([result_ehr_prg, result_ehr_overall]).sort_values(['ehr_name', 'relevance'], ascending=[True, False])\n",
    "\n",
    "    # Table with scores for each EHR, Hospital Name groups, and relevance groups --> specifically for table in manuscript supplementary file\n",
    "    result_ehr_hosp = proc.loc[proc['rank'] == rank].groupby(['ehr_name', 'hospital_name']).apply(lambda x: calculate_scores_for_groups(data=x, label_true=TARGET, label_pred='label'))\n",
    "    result_ehr_hosp['relevance'] = (result_ehr_hosp['concept_label'] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    result_ehr_hosp_prg = result_ehr_hosp.reset_index().groupby(['ehr_name', 'hospital_name', 'relevance']).apply(lambda x: weighted_average(x)).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    # combine relevant and irrelevant into a weighted average overall score\n",
    "    result_ehr_hosp_overall = result_ehr_hosp_prg.reset_index().groupby(['ehr_name', 'hospital_name']).apply(\n",
    "        lambda x: weighted_average(x)\n",
    "            ).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    result_ehr_hosp_overall = result_ehr_hosp_overall.reset_index()\n",
    "    result_ehr_hosp_overall['relevance'] = 'zoverall'\n",
    "    result_ehr_hosp_overall.set_index(['ehr_name', 'hospital_name', 'relevance'], inplace=True)\n",
    "    result_ehr_hosp_prg = pd.concat([result_ehr_hosp_prg, result_ehr_hosp_overall]).sort_values(['ehr_name', 'hospital_name', 'relevance'], ascending=[True, True, False])\n",
    "\n",
    "    return {'label': result,\n",
    "            'label_group': result_per_concept_label_group,\n",
    "            'relevance': result_per_relevance_group,\n",
    "            'ehr_relevance': result_ehr_prg,\n",
    "            'ehr_hosp_relevance': result_ehr_hosp_prg,\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_data(results: Dict[int, Dict[str, pd.DataFrame]],\n",
    "                  dataset: str = 'relevance',\n",
    "                  score_type='recall',\n",
    "                  ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    y_values = pd.concat([results[i][dataset][score_type] for i in range(1, 11)], axis=1)\n",
    "    y_values.columns = list(range(1,11))\n",
    "\n",
    "    s_values = pd.concat([results[i][dataset]['support'] for i in range(1, 11)], axis=1)\n",
    "    s_values.columns = list(range(1,11))\n",
    "    s_values = s_values.astype(int)\n",
    "    return y_values.transpose(), s_values.transpose()\n",
    "\n",
    "def plot_results(results: Dict[int, Dict[str, pd.DataFrame]],\n",
    "                 dataset: str = 'relevance',\n",
    "                 score_type: str = 'recall',\n",
    "                 N: int = 10,\n",
    "                 cumulative: bool = True,\n",
    "                 plot_order=None,\n",
    "                 color_palette=None,\n",
    "                 save_loc=None,\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Plots the results of the evaluation.\n",
    "    :param results:\n",
    "    :param score_type:\n",
    "    :param N:\n",
    "    :param cumulative:\n",
    "    :param plot_order:\n",
    "    :param color_palette:\n",
    "    :param save_loc:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    score_data, count_data = get_plot_data(results=results, dataset=dataset, score_type=score_type)\n",
    "\n",
    "    if cumulative:\n",
    "        score_data = score_data.cumsum()\n",
    "    score_data.to_csv(f'{save_loc}score_data__{dataset}__{score_type}.csv')\n",
    "    count_data.to_csv(f'{save_loc}count_data__{dataset}__{score_type}.csv')\n",
    "    plot_order = sorted(score_data.columns, reverse=True) if plot_order is None else plot_order\n",
    "    plot_order_rename_dict = {x: x.replace('zoverall', 'overall').capitalize() for x in plot_order}\n",
    "    score_data.rename(columns=plot_order_rename_dict, inplace=True)\n",
    "    count_data.rename(columns=plot_order_rename_dict, inplace=True)\n",
    "    plot_order = plot_order_rename_dict.values()\n",
    "    c_palette = ['black'] * len(plot_order) if color_palette is None else color_palette[0:len(plot_order)] #['black'] * count_values.shape[1]\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1,\n",
    "                                   sharex=True,\n",
    "                                   figsize=(6,6),\n",
    "                                   gridspec_kw={'height_ratios': [6,2],\n",
    "                                                },\n",
    "                                   )\n",
    "\n",
    "    # Plot scores\n",
    "    sns.lineplot(data=score_data[plot_order],\n",
    "                 palette=c_palette,\n",
    "                 legend=True,\n",
    "                 ax=ax1,\n",
    "                 )\n",
    "    ax1.set_xlabel('Rank of predicted labels')\n",
    "    ax1.set_ylabel(f\"{score_type}\".capitalize(), labelpad=25)\n",
    "    ax1.set_ylim(0, 1.01)\n",
    "    ax1.set_xlim(1, N)\n",
    "    ax1.legend(\n",
    "        loc='lower right',\n",
    "        bbox_to_anchor=(1.0, 0.0),\n",
    "        ncol=1,\n",
    "    )\n",
    "\n",
    "    # Plot parameter counts\n",
    "    sns.lineplot(data=count_data[plot_order],\n",
    "                 palette=c_palette,\n",
    "                 legend=False,\n",
    "                 ax=ax2,\n",
    "                 )\n",
    "    ax2.set_ylim(0, count_data.max().max()*1.05)\n",
    "    ax2.set_xlim(1, N)\n",
    "    ax2.set_xticks(list(range(1, N+1)))\n",
    "    ax2.set_xticklabels(list(range(1, N+1)))\n",
    "    ax2.set_xlabel('Number of predicted labels')\n",
    "    ax2.set_ylabel('Parameter\\ncount')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_loc}plot__{dataset}__{score_type}.png\", dpi=1200)\n",
    "    plt.savefig(f\"{save_loc}plot__{dataset}__{score_type}.pdf\", dpi=1200)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_concept_grouping(data, source, target) -> dict:\n",
    "    assert data[source].duplicated().sum() == 0, 'source column is not unique'\n",
    "    return data.set_index(source)[target].to_dict()\n",
    "\n",
    "concepts = pd.read_csv('../data/input/concepts.csv')\n",
    "concept_category_groups = create_concept_grouping(concepts, 'concept_label', 'category')\n",
    "concept_label_super_groups = create_concept_grouping(concepts, 'concept_label', 'concept_label_super')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = pd.read_csv('../data/input/combined.csv')\n",
    "mappings[\"id\"] = mappings.reset_index(drop=True).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am = AutoMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_dict = {'stantonius': 'EPIC',\n",
    "            'amc': 'EPIC',\n",
    "            'vumc': 'EPIC',\n",
    "            'etz': 'EPIC',\n",
    "            'radboud': 'EPIC',\n",
    "            'spaarnegasthuis': 'EPIC',\n",
    "            'catharina': 'HIX',\n",
    "            'cwz': 'HIX',\n",
    "            'ikazia': 'HIX',\n",
    "            'martini': 'HIX',\n",
    "            'slingeland': 'HIX',\n",
    "            'erasmus': 'HIX',\n",
    "            'bovenij': 'HIX',\n",
    "            'noordwest': 'HIX',\n",
    "            'franciscus': 'HIX',\n",
    "            'viecuri': 'HIX',\n",
    "            'zgt': 'HIX',\n",
    "            'rdgg': 'HIX',\n",
    "            'laurentius': 'HIX',\n",
    "            'haga': 'MV',\n",
    "            'mst': 'MV',\n",
    "            'umcu': 'MV',\n",
    "            'albertschweitzer': 'MV',\n",
    "            'maasstad': 'MV',\n",
    "            'zuyderland': 'MV',\n",
    "            'olvg': 'MV',\n",
    "            'amphia': 'MV',\n",
    "            'jeroenbosch': 'MV',\n",
    "            'geldersevallei': 'MV',\n",
    "            'antoniuszorggroep': 'MV'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False  # Expected training duration 3 hours\n",
    "results = dict()\n",
    "if load:\n",
    "    try:\n",
    "        results = joblib.load('./results.joblib')\n",
    "    except:\n",
    "        files = glob.glob('./output/all_mapped/results/*.csv')\n",
    "        for file in files:\n",
    "            results[file.split('\\\\')[-1].split('.')[0]] = pd.read_csv(file)\n",
    "else:\n",
    "    results = {}\n",
    "    t_start = datetime.now()\n",
    "    t_length = ehr_dict.keys().__len__()\n",
    "    for i, hospital in enumerate(ehr_dict.keys()):\n",
    "        print(f\"Starting {hospital} {i+1}/{t_length} {datetime.now() - t_start}\")\n",
    "        print(f\"Expected duration: {((datetime.now() - t_start) / (i+1)) * (t_length - (i+1))}\")\n",
    "\n",
    "        test_data = mappings.loc[mappings['hospital_name'] == hospital]\n",
    "        train_data = mappings.loc[(mappings['hospital_name'] != hospital) & (mappings[TARGET] != 'unmapped')]\n",
    "        print(f\"Testing on {hospital} {test_data.shape} and training on {train_data['hospital_name'].unique()} {train_data.shape}\")\n",
    "\n",
    "        # Create the pipe\n",
    "        am.create_pipe(X=train_data['parameter_name'], y=train_data['concept_label'], n_jobs=-1)\n",
    "        print(f\"Pipe created, predicting labels\")\n",
    "        results[hospital] = am.predict_proba_transformed(test_data[['id', 'parameter_name']])\n",
    "    try:\n",
    "        os.makedirs('./output/all_mapped_elasticnet/results/', exist_ok=True)\n",
    "        for key, value in results.items():\n",
    "            print(key)\n",
    "            value.to_csv(f\"./output/all_mapped_elasticnet/results/{key}.csv\", index=False)\n",
    "        print('csvs written')\n",
    "    except:\n",
    "        joblib.dump(results, './results.joblib')\n",
    "        print('joblib dumped')\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in results.items():\n",
    "    results[key]['hospital'] = key\n",
    "df = pd.concat([x for x in results.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['concept_label_original'] = df['id'].map(mappings.set_index('id')['concept_label'])\n",
    "df['correct'] = df['concept_label_original'] == df['label']\n",
    "df = df.sort_values(['id', 'value'], ascending=[True, False])\n",
    "df['rank'] = df.groupby('id').cumcount() + 1\n",
    "df['ehr'] = df['id'].map(mappings.set_index('id')['ehr_name'])\n",
    "df['relevance'] = (df['concept_label_original'] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular performance plot and tables\n",
    "save_loc='./output/all_mapped_elasticnet/loo/'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "\n",
    "predicted_labels = df.copy()\n",
    "test_data = mappings.copy()\n",
    "\n",
    "\n",
    "results = dict()\n",
    "for i in range(1, 11):\n",
    "    results[i] = calculate_scores(predicted_labels, test_data[['id', 'hospital_name' , 'ehr_name', 'concept_label', 'num_records']], {TARGET: concept_category_groups}, rank=i)\n",
    "    for key, value in results[i].items():\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            for col in value.columns:\n",
    "                if value[col].dtype == 'float64':\n",
    "                    if (value[col] == value[col].astype(int).astype(float)).all():\n",
    "                        value[col] = value[col].astype(int)\n",
    "            value.to_csv(f'{save_loc}aprf__rank_{i}__{key}.csv')\n",
    "            value.round(3).to_csv(f'{save_loc}aprf__rank_{i}__{key}__round3.csv', float_format='%.3f')\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('paper', font_scale=1)\n",
    "plot_results(results, score_type='precision', save_loc=save_loc)\n",
    "plot_results(results, score_type='recall', save_loc=save_loc)\n",
    "plot_results(results, score_type='f1', save_loc=save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catplot / swarmplot\n",
    "\n",
    "# calculate accuracy per hospital/EHR\n",
    "a = df.groupby(['ehr', 'hospital', 'rank', 'relevance'])['correct'].mean()\n",
    "\n",
    "# calculate overall score\n",
    "b = df.groupby(['ehr', 'hospital', 'rank'])['correct'].mean()\n",
    "b = pd.DataFrame(b)\n",
    "b['relevance'] = 'overall'\n",
    "b = b.groupby(['ehr', 'hospital', 'rank', 'relevance']).mean()\n",
    "b.columns = [0]\n",
    "\n",
    "\n",
    "c = pd.concat([a, b])\n",
    "c.sort_values(['ehr', 'hospital', 'rank', 'relevance'], inplace=True)\n",
    "\n",
    "d = c.groupby(['ehr', 'hospital', 'relevance']).cumsum().unstack()\n",
    "d.columns = ['irrelevant', 'overall', 'relevant']\n",
    "d = d.reset_index()\n",
    "d = d.melt(\n",
    "    id_vars=['ehr', 'hospital', 'rank'],\n",
    "    value_vars=['irrelevant', 'overall', 'relevant'],\n",
    ")\n",
    "d = d.rename(columns={'ehr': 'EHR', 'value': 'Recall', 'variable': 'Relevance', 'rank': 'Number of predicted labels',})\n",
    "d['Relevance'] = d['Relevance'].str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_context('paper', font_scale=2)\n",
    "sns.catplot(\n",
    "    data=d.loc[d['Number of predicted labels'] <= 10],\n",
    "    x='Number of predicted labels',\n",
    "    y='Recall',\n",
    "    color='black',\n",
    "    col_order=['Overall', 'Relevant', 'Irrelevant'],\n",
    "    row_order=['EPIC', 'HIX', 'MV'],\n",
    "    row='EHR',\n",
    "    col='Relevance',\n",
    "    kind='swarm',\n",
    "    sharex=True, sharey=True,\n",
    "    facet_kws={\n",
    "        'xlim': (1, 10),\n",
    "        'ylim':(0,1.01),\n",
    "    },\n",
    "    **{'height':8},\n",
    ")\n",
    "\n",
    "plt.savefig(f\"{save_loc}grid_plot__ehr__relevance__recall.png\", dpi=1200)\n",
    "plt.savefig(f\"{save_loc}grid_plot__ehr__relevance__recall.pdf\", dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
