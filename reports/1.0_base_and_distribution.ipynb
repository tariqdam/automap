{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Distribution Evaluation of AutoMap\n",
    "We are going to evaluate whether we can approve the accuracy of the most probable predicted class by adjusting the predicted probabilities on parameter names based on similarity to already mapped concepts.\n",
    "\n",
    "This means we need to retrieve the original mapped data from disk which is already processed by hand to ensure uniformity and results in a table with at least columns:\n",
    "- `parameter_name`, `concept_label`\n",
    "- `p25`, `p50`, `p75`, `num_records` for numeric data\n",
    "- an entry in `atc` for medication records if available\n",
    "\n",
    "### Concept labels\n",
    "Concept labels are retrieved from the general parameters table. A concept table is also loaded which contains the category structure for the labels for stratified analysis.\n",
    "Output: dictionary concept_groups: {concept_label: category}\n",
    "Output: dictionary concept_label_super_groups {concept_label: concept_label_super}\n",
    "\n",
    "\n",
    "### Reference groups\n",
    "A reference group is a set of previously mapped parameters which describe the expected content of a concept. Instead of defining extreme values for concepts, we retrieve reference values from actual measurements. This is a good idea because extreme values would need to be defined for each concept which is tedious work and up for debate, while using actual measurements may guide in sorting on subtle differences in measured values such as those observed between measured tidal volumes between inspiration and expiration. For each concept's reference group, matching parameters are retrieved and their data is pooled by taking a weighted average of the percentiles based on the number of records. This way, a parameter with a small sample size will have less influence on the overall shape of the data distribution.\n",
    "Note: another approach may be to take the minimum of the p25, the weighted average for p50, and the max of the p75, for parameters with at least N records.\n",
    "Output: nested dictionary of concept labels: {p25: float, p50: float, p75: float, N: int}\n",
    "\n",
    "### Distribution\n",
    "Distribution of underlying data is compared based on the 25th, 50th and 75th percentile, also known as the median and lower and upper quartiles. In an earlier attempt, we used a modified T-statistic to calculate similarity and increase the probability of predicted labels if data distribution was similar. If the T-statisic was low, the increase was higher than when the T-statistic was high, so that concepts with closer reference groups would become more visible. However, this lead to a lot of false positives and tanked the predictions. Therefore, we will now attempt to not select on similarity, but to deselect on dissimilarity by discounting predictions where the reference group is deemed to be dissimilar.\n",
    "Output: Function(parameter_id, predicted_concept_label, parameter_distributions, reference_groups) -> similarity statistic\n",
    "Output: Function(similarity_statistic, probability_of_predicted_concept_label_for_this_parameter) -> probability_of_predicted_concept_label_for_this_parameter || 0\n",
    "Evaluation of performance change compared to base model at major group level\n",
    "\n",
    "### Record type\n",
    "For medication, we can set the probability for non-medication labels to `0` if the parameter contains an `ATC` value. This requires `concept_labels` to contain a grouping structure which labels the concepts as ATC-linked concepts. This can be either achieved through manual labeling of each concept (ideal, but a lot of work), or through the labeling of the concepts based on earlier mapped parameters. For practical purposes, we can assume all concept_labels starting with `med_*` to be medication `concept_labels`. Using the ATC to do a look-up for the concept_label is not preferred, as ATC codes are provided by hospitals and may not be validated as regularly seen during the mapping of the covid project.\n",
    "Output: Function(parameter_id, predicted_concept_label, parameter_distributions) -> parameter_has_atc_and_predicted_concept_is_medication as bool\n",
    "Output: Function(parameter_has_atc_and_predicted_concept_is_medication, probability_of_predicted_concept_label_for_this_parameter) -> probability_of_predicted_concept_label_for_this_parameter || 0\n",
    "Output: Evaluation of performance change compared to base model at major group level\n",
    "Output: Evaluation of performance change compared to base model + distribution at major group level\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "To evaluate the performance of this model, we collect the accuracy, precision, recall and F1-score at concept_label level (tidal_volume_measured_ventilator) without stratifying, as well as accuracy stratified per relevance for relevant/irrelevant records, as well as stratified for major groups (hemodynamic, respiratory, medication), and stratified for minor groups (tidal volume, heart rate) where these are available in the grouping structure. The accuracy is plotted as a cumulative accuracy over the number of predicted labels sorted by their probability, both for the concept_label level and lines for the relevant/irrelevant parameters, over the number of parameters which still receive predicted labels at label position N. The precision, recall and F1-scores are reported as tables with y-values corresponding to the stratified labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple, List, Dict, Union\n",
    "\n",
    "import sklearn\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context(\"paper\", font_scale = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_HOSPITALS = ['vumc', 'amc', 'erasmus', 'olvg']\n",
    "EHR_SYSTEMS = ['epic', 'mv', 'hix']\n",
    "SOURCE = 'parameter_name'\n",
    "TARGET = 'concept_label'\n",
    "HOSPITAL_COLUMN = 'hospital_name'\n",
    "DATA_DISTRIBUTION_COLUMNS = ['amin', 'amax', 'p25', 'p50', 'p75', 'p50_over_iqr', 'iqr_over_p50', 'skewed']\n",
    "DATA_DISTRIBUTION_WEIGHTS = 'num_records'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def calculate_performance(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': sklearn.metrics.accuracy_score(y_true, y_pred),\n",
    "        'precision': sklearn.metrics.precision_score(y_true, y_pred, average='weighted'),\n",
    "        'recall': sklearn.metrics.recall_score(y_true, y_pred, average='weighted'),\n",
    "        'f1_score': sklearn.metrics.f1_score(y_true, y_pred, average='weighted'),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_predictions_to_proportions(predictions: pd.DataFrame,\n",
    "                                         original_data: pd.DataFrame,\n",
    "                                         cumulative_score: bool = False,\n",
    "                                         ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Transform probability predictions from the AutoMap class to proportions of correct predictions per rank stratified over relevance, irrelevance and overall scores to be used for plotting.\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param original_data: pandas DataFrame of the original data as input to the AutoMap class, where each row represents a document and its corresponding true concept labels.\n",
    "    :param cumulative_score: boolean indicating whether to calculate cumulative scores over the top X predictions.\n",
    "    :return: tuple of pandas DataFrames where the first returns the probability while the second returns the number of parameters for which predictions were made\n",
    "    \"\"\"\n",
    "\n",
    "    _predictions = predictions.sort_values(['id', 'value'], ascending=[True, False])\n",
    "    _predictions[TARGET] = _predictions['id'].map(original_data.set_index('id')[TARGET])\n",
    "    _predictions['rank'] = _predictions.groupby('id').cumcount()\n",
    "    _predictions['rank_correct'] = (_predictions['label'] == _predictions[TARGET]).astype(int)\n",
    "    _predictions['relevance'] = (_predictions[TARGET] == 'unmapped').map({True: 'irrelevant',\n",
    "                                                                          False: 'relevant'})\n",
    "    # calculate scores\n",
    "    scores = _predictions.groupby(['relevance', 'rank'])['rank_correct'].sum().reset_index()\n",
    "    scores_plot = scores.pivot(index=['rank'], columns=['relevance'], values=['rank_correct']).fillna(0)\n",
    "    scores_plot.columns = scores_plot.columns.droplevel()\n",
    "    scores_plot['overall'] = scores_plot.sum(axis=1)\n",
    "    if cumulative_score:\n",
    "        scores_plot = scores_plot.cumsum()\n",
    "    scores_plot = scores_plot[sorted(scores_plot.columns)]\n",
    "\n",
    "    # get the number of parameter in each group of relevance\n",
    "    parameter_count = _predictions[['id', 'relevance']].groupby(['relevance'])['id'].nunique()\n",
    "    parameter_count['overall'] = parameter_count.sum()\n",
    "    parameter_count = parameter_count.sort_index()\n",
    "\n",
    "    scores_plot_ratio = scores_plot / parameter_count\n",
    "    print(scores_plot_ratio)\n",
    "\n",
    "    parameter_count = _predictions[['id', 'relevance', 'rank']].groupby(['relevance', 'rank'])['id'].count()\n",
    "    parameter_count = parameter_count.reset_index().pivot(index='rank', columns='relevance', values='id')\n",
    "    parameter_count['overall'] = parameter_count['irrelevant'].fillna(0) + parameter_count['relevant'].fillna(0)\n",
    "    print(parameter_count)\n",
    "\n",
    "    return scores_plot_ratio, parameter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge predictions with original data to retrieve grouping categories\n",
    "def merge_predictions_with_original_data(predictions: pd.DataFrame,\n",
    "                                         original_data: pd.DataFrame,\n",
    "                                         grouping_categories: Dict[str, Dict[str, str]] = None,\n",
    "                                         ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get grouping categories for predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param original_data: pandas DataFrame of the original data as input to the AutoMap class, where each row represents a document and its corresponding true concept labels.\n",
    "    :param grouping_categories: dictionary of column name followed by dictionary to map source to target values\n",
    "    :return: pandas DataFrame with predictions and grouping categories\n",
    "    \"\"\"\n",
    "    _predictions = predictions.sort_values(['id', 'value']).copy()\n",
    "    _predictions = _predictions.merge(original_data, on='id')\n",
    "    if grouping_categories:\n",
    "        for key, values in grouping_categories.items():\n",
    "            _predictions[f\"{key}_groups\"] = _predictions[key].map(values)\n",
    "    return _predictions\n",
    "\n",
    "# assign correct flag to predictions\n",
    "def assign_correct_flag(predictions: pd.DataFrame,\n",
    "                        target_column: str = TARGET,\n",
    "                        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign correct flag to predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param target_column: column name of the target column\n",
    "    :return: pandas DataFrame with predictions and correct flag\n",
    "    \"\"\"\n",
    "    _predictions = predictions.copy()\n",
    "    _predictions['correct'] = _predictions[target_column] == _predictions['label']\n",
    "    return _predictions\n",
    "\n",
    "# assign ranks to predictions\n",
    "def assign_ranks(predictions: pd.DataFrame,\n",
    "                 target_column: str = 'label',\n",
    "                 ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign ranks to predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param target_column: column name of the target column\n",
    "    :return: pandas DataFrame with predictions and ranks\n",
    "    \"\"\"\n",
    "    _predictions = predictions.sort_values(['id', 'value'], ascending=[True, False]).copy()\n",
    "    _predictions['rank'] = _predictions.groupby('id').cumcount() + 1\n",
    "    return _predictions\n",
    "\n",
    "\n",
    "def assign_relevance(predictions: pd.DataFrame,\n",
    "                     target_column: str = TARGET,\n",
    "                     ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign relevance to predictions\n",
    "\n",
    "    :param predictions: pandas DataFrame of probability predictions as output by the AutoMap class, where each row represents a prediction for a concept for a document and its corresponding probability.\n",
    "    :param target_column: column name of the target column\n",
    "    :return: pandas DataFrame with predictions and relevance\n",
    "    \"\"\"\n",
    "    _predictions = predictions.copy()\n",
    "    _predictions['relevance'] = (_predictions[target_column] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    return _predictions\n",
    "\n",
    "def get_processed_data(predictions,\n",
    "                       original_data,\n",
    "                       grouping_categories,\n",
    "                       ):\n",
    "\n",
    "    _predictions = merge_predictions_with_original_data(predictions=predictions,\n",
    "                                                        original_data=original_data,\n",
    "                                                        grouping_categories=grouping_categories)\n",
    "    _predictions = assign_correct_flag(predictions=_predictions)\n",
    "    _predictions = assign_ranks(predictions=_predictions)\n",
    "    _predictions = assign_relevance(predictions=_predictions)\n",
    "    return _predictions\n",
    "\n",
    "\n",
    "def calculate_scores_for_groups(\n",
    "        data: pd.DataFrame,\n",
    "        label_true: str = TARGET,\n",
    "        label_pred: str = 'label',\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate scores for the data passed in.\n",
    "    :param data: pandas DataFrame with at least columns for true labels and predicted labels\n",
    "    :param label_true: string with the name of the column containing the true labels\n",
    "    :param label_pred: string with the name of the column containing the predicted labels\n",
    "    :return: pandas DataFrame with scores for each true label\n",
    "    \"\"\"\n",
    "\n",
    "    result = pd.DataFrame(sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_true=data[label_true],\n",
    "        y_pred=data[label_pred],\n",
    "        labels=data[label_true].unique(),\n",
    "        average=None, #average='weighted',\n",
    "        beta=1,\n",
    "        zero_division=0,\n",
    "    )).transpose().set_index(data[label_true].unique())\n",
    "    result.columns = ['precision', 'recall', 'f1', 'support']\n",
    "    result.index.name = label_true\n",
    "    result = result.reset_index()\n",
    "\n",
    "    result_accuracy = data.groupby([TARGET]).apply(lambda x: sklearn.metrics.accuracy_score(\n",
    "        y_true=x[label_true],\n",
    "        y_pred=x[label_pred],\n",
    "        normalize=True,\n",
    "        sample_weight=None,\n",
    "    )).to_dict()\n",
    "    result['accuracy'] = result[label_true].map(result_accuracy)\n",
    "\n",
    "    result_num_records = data.groupby([TARGET]).apply(lambda x: np.sum(x['num_records'])).to_dict()\n",
    "    result['num_records'] = result[label_true].map(result_num_records)\n",
    "\n",
    "    return result\n",
    "\n",
    "def weighted_average(x: pd.DataFrame,\n",
    "                     score_types: List[str] = None,\n",
    "                     ) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculated the weighted average for each score type if they're present in the dataframe columns. Expects 'support' to contain counts for each score type.\n",
    "    :param x: pandas DataFrame\n",
    "    :param score_types: list of strings with score types to calculate weighted average for\n",
    "    :return: dictionary with weighted average for each score type\n",
    "    \"\"\"\n",
    "    score_types = ['accuracy', 'precision', 'recall', 'f1'] if score_types is None else score_types\n",
    "    sum_cols = ['num_records', 'support', 'group_count']\n",
    "    return_dict = {score_type: np.average(x[score_type], weights=x['support']) for score_type in score_types if score_type in x}\n",
    "    for col in sum_cols:\n",
    "        if col in x:\n",
    "            return_dict[col] = int(np.sum(x[col]))\n",
    "        elif col == 'group_count':\n",
    "            return_dict[col] = len(x)\n",
    "    return return_dict\n",
    "\n",
    "def calculate_scores(predictions: pd.DataFrame,\n",
    "                     original_data: pd.DataFrame,\n",
    "                     grouping_categories: dict = None,\n",
    "                     rank: int = 1,\n",
    "                     ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Retrieves various grouped scores for the publication.\n",
    "    :param predictions: pandas DataFrame of the predictions with at least the columns 'id', 'label', 'value'\n",
    "    :param original_data: panda DataFrame of the original data being predicted on. Must contain the columns 'id', TARGET concept label and ehr_name.\n",
    "    :param grouping_categories: dictionary of column names and a corresponding dictionary to map values to. Groups will be written to {key}_group column.\n",
    "    :param rank: integer of the rank to calculate scores for, default is 1 for the first prediction\n",
    "    :return: dictionary of various grouping structures and the respective table for accuracy/precision/recall/f1-scores and support\n",
    "    \"\"\"\n",
    "    proc = get_processed_data(predictions=predictions,\n",
    "                              original_data=original_data,\n",
    "                              grouping_categories=grouping_categories)\n",
    "    # Table with scores for each concept label --> allows for grouping over data categories and relevance\n",
    "    result = calculate_scores_for_groups(data=proc.loc[proc['rank'] == rank], label_true=TARGET, label_pred='label')\n",
    "    result['concept_label_group'] = result['concept_label'].map(concept_category_groups)\n",
    "    result['relevance'] = (result['concept_label'] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    result = result.loc[result['support'] > 0].copy() # remove concepts that were not available in the test set as they cannot be evaluated\n",
    "\n",
    "    # Table with scores for each concept label group\n",
    "    result_per_concept_label_group = result.groupby(['concept_label_group']).apply(lambda x: weighted_average(x)\n",
    "            ).apply(pd.Series).sort_values('support', ascending=False)\n",
    "\n",
    "    # Table with scores for each relevance group\n",
    "    result_per_relevance_group = result.groupby(['relevance']).apply(lambda x: weighted_average(x)).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    result_prg_overall = weighted_average(result_per_relevance_group.reset_index())\n",
    "    result_prg_overall = pd.DataFrame(result_prg_overall, index=['zoverall'])\n",
    "    result_per_relevance_group = pd.concat([result_prg_overall, result_per_relevance_group]).sort_index(ascending=False)\n",
    "\n",
    "    # Table with scores for each EHR system and Relevance groups --> specifically for table in manuscript\n",
    "    result_ehr = proc.loc[proc['rank'] == rank].groupby(['ehr_name']).apply(lambda x: calculate_scores_for_groups(data=x, label_true=TARGET, label_pred='label'))\n",
    "    result_ehr['relevance'] = (result_ehr['concept_label'] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    result_ehr_prg = result_ehr.reset_index().groupby(['ehr_name', 'relevance']).apply(lambda x: weighted_average(x)).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    # combine relevant and irrelevant into a weighted average overall score\n",
    "    result_ehr_overall = result_ehr_prg.reset_index().groupby(['ehr_name']).apply(\n",
    "        lambda x: weighted_average(x)\n",
    "            ).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    result_ehr_overall = result_ehr_overall.reset_index()\n",
    "    result_ehr_overall['relevance'] = 'zoverall'\n",
    "    result_ehr_overall.set_index(['ehr_name', 'relevance'], inplace=True)\n",
    "    result_ehr_prg = pd.concat([result_ehr_prg, result_ehr_overall]).sort_values(['ehr_name', 'relevance'], ascending=[True, False])\n",
    "\n",
    "    # Table with scores for each EHR, Hospital Name groups, and relevance groups --> specifically for table in manuscript supplementary file\n",
    "    result_ehr_hosp = proc.loc[proc['rank'] == rank].groupby(['ehr_name', 'hospital_name']).apply(lambda x: calculate_scores_for_groups(data=x, label_true=TARGET, label_pred='label'))\n",
    "    result_ehr_hosp['relevance'] = (result_ehr_hosp['concept_label'] == 'unmapped').map({True: 'irrelevant', False: 'relevant'})\n",
    "    result_ehr_hosp_prg = result_ehr_hosp.reset_index().groupby(['ehr_name', 'hospital_name', 'relevance']).apply(lambda x: weighted_average(x)).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    # combine relevant and irrelevant into a weighted average overall score\n",
    "    result_ehr_hosp_overall = result_ehr_hosp_prg.reset_index().groupby(['ehr_name', 'hospital_name']).apply(\n",
    "        lambda x: weighted_average(x)\n",
    "            ).apply(pd.Series).sort_values('support', ascending=False)\n",
    "    result_ehr_hosp_overall = result_ehr_hosp_overall.reset_index()\n",
    "    result_ehr_hosp_overall['relevance'] = 'zoverall'\n",
    "    result_ehr_hosp_overall.set_index(['ehr_name', 'hospital_name', 'relevance'], inplace=True)\n",
    "    result_ehr_hosp_prg = pd.concat([result_ehr_hosp_prg, result_ehr_hosp_overall]).sort_values(['ehr_name', 'hospital_name', 'relevance'], ascending=[True, True, False])\n",
    "\n",
    "    return {'label': result,\n",
    "            'label_group': result_per_concept_label_group,\n",
    "            'relevance': result_per_relevance_group,\n",
    "            'ehr_relevance': result_ehr_prg,\n",
    "            'ehr_hosp_relevance': result_ehr_hosp_prg,\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_data(results: Dict[int, Dict[str, pd.DataFrame]],\n",
    "                  dataset: str = 'relevance',\n",
    "                  score_type='recall',\n",
    "                  ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    y_values = pd.concat([results[i][dataset][score_type] for i in range(1, 11)], axis=1)\n",
    "    y_values.columns = list(range(1,11))\n",
    "\n",
    "    s_values = pd.concat([results[i][dataset]['support'] for i in range(1, 11)], axis=1)\n",
    "    s_values.columns = list(range(1,11))\n",
    "    s_values = s_values.astype(int)\n",
    "    return y_values.transpose(), s_values.transpose()\n",
    "\n",
    "def plot_results(results: Dict[int, Dict[str, pd.DataFrame]],\n",
    "                 dataset: str = 'relevance',\n",
    "                 score_type: str = 'recall',\n",
    "                 N: int = 10,\n",
    "                 cumulative: bool = True,\n",
    "                 plot_order=None,\n",
    "                 color_palette=None,\n",
    "                 save_loc=None,\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Plots the results of the evaluation.\n",
    "    :param results:\n",
    "    :param score_type:\n",
    "    :param N:\n",
    "    :param cumulative:\n",
    "    :param plot_order:\n",
    "    :param color_palette:\n",
    "    :param save_loc:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    score_data, count_data = get_plot_data(results=results, dataset=dataset, score_type=score_type)\n",
    "\n",
    "    if cumulative:\n",
    "        score_data = score_data.cumsum()\n",
    "    score_data.to_csv(f'{save_loc}score_data__{dataset}__{score_type}.csv')\n",
    "    count_data.to_csv(f'{save_loc}count_data__{dataset}__{score_type}.csv')\n",
    "    plot_order = sorted(score_data.columns, reverse=True) if plot_order is None else plot_order\n",
    "    plot_order_rename_dict = {x: x.replace('zoverall', 'overall').capitalize() for x in plot_order}\n",
    "    score_data.rename(columns=plot_order_rename_dict, inplace=True)\n",
    "    count_data.rename(columns=plot_order_rename_dict, inplace=True)\n",
    "    plot_order = plot_order_rename_dict.values()\n",
    "    c_palette = ['black'] * len(plot_order) if color_palette is None else color_palette[0:len(plot_order)] #['black'] * count_values.shape[1]\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1,\n",
    "                                   sharex=True,\n",
    "                                   figsize=(6,6),\n",
    "                                   gridspec_kw={'height_ratios': [6,2],\n",
    "                                                },\n",
    "                                   )\n",
    "\n",
    "    # Plot scores\n",
    "    sns.lineplot(data=score_data[plot_order],\n",
    "                 palette=c_palette,\n",
    "                 legend=True,\n",
    "                 ax=ax1,\n",
    "                 )\n",
    "    ax1.set_xlabel('Rank of predicted labels')\n",
    "    ax1.set_ylabel(f\"{score_type}\".capitalize(), labelpad=25)\n",
    "    ax1.set_ylim(0, 1.01)\n",
    "    ax1.set_xlim(1, N)\n",
    "    ax1.legend(\n",
    "        loc='lower right',\n",
    "        bbox_to_anchor=(1.0, 0.0),\n",
    "        ncol=1,\n",
    "    )\n",
    "\n",
    "    # Plot parameter counts\n",
    "    sns.lineplot(data=count_data[plot_order],\n",
    "                 palette=c_palette,\n",
    "                 legend=False,\n",
    "                 ax=ax2,\n",
    "                 )\n",
    "    ax2.set_ylim(0, count_data.max().max()*1.05)\n",
    "    ax2.set_xlim(1, N)\n",
    "    ax2.set_xticks(list(range(1, N+1)))\n",
    "    ax2.set_xticklabels(list(range(1, N+1)))\n",
    "    ax2.set_xlabel('Number of predicted labels')\n",
    "    ax2.set_ylabel('Parameter\\ncount')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_loc}plot__{dataset}__{score_type}.png\", dpi=1200)\n",
    "    plt.savefig(f\"{save_loc}plot__{dataset}__{score_type}.pdf\", dpi=1200)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get concept groupings\n",
    "\n",
    "Concept groups are dictionaries which translate individual concept labels to their respective groups to enable reporting on performance stratified over major categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_concept_grouping(data, source, target) -> dict:\n",
    "    assert data[source].duplicated().sum() == 0, 'source column is not unique'\n",
    "    return data.set_index(source)[target].to_dict()\n",
    "\n",
    "concepts = pd.read_csv('../data/input/concepts.csv')\n",
    "concept_category_groups = create_concept_grouping(concepts, 'concept_label', 'category')\n",
    "concept_label_super_groups = create_concept_grouping(concepts, 'concept_label', 'concept_label_super')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get reference groups\n",
    "\n",
    "Reference groups are concept labels which use the training hospital's mapped parameters to retrieve an average value for their underlying data content. Reference groups contain averages for p25, p50, p75, weighted by the number of records per parameter, as well as normalized values for their distribution, such as p50_over_iqr and iqr_over_p50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = pd.read_csv('../data/input/combined.csv',\n",
    "                       usecols=['parameter_name', 'concept_label', 'hospital_name', 'ehr_name', 'atc', 'amin', 'amax', 'p25', 'p50', 'p75', 'num_records', 'unit'],\n",
    "                       )\n",
    "mappings[\"id\"] = mappings.reset_index(drop=True).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings['iqr'] = mappings['p75'] - mappings['p25']\n",
    "mappings['p50_over_iqr'] = mappings.p50 / mappings.iqr\n",
    "mappings['iqr_over_p50'] = mappings.iqr / mappings.p50\n",
    "mappings['skewed'] = (mappings['p50'] - mappings['p25']) / mappings['iqr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings.loc[mappings['concept_label'].str.contains('o2_arterial')].groupby('concept_label')[['p25', 'p50', 'p75', 'p50_over_iqr', 'iqr_over_p50', 'skewed']].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_groups(data: pd.DataFrame,\n",
    "                            group_by: Union[List[str], str],\n",
    "                            filter_by: List[str],\n",
    "                            filter_on: List[str],\n",
    "                            values: List[str],\n",
    "                            weights: str,\n",
    "                            ) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Create reference groups for a given set of parameters.\n",
    "    :param data:\n",
    "    :param group_by:\n",
    "    :param filter_by:\n",
    "    :param filter_on:\n",
    "    :param values:\n",
    "    :param weights:\n",
    "    :return: dictionary with keys corresponding to concept labels\n",
    "    \"\"\"\n",
    "    var = data.loc[data[filter_by].isin(filter_on)].groupby(group_by, group_keys=False)\n",
    "    d = dict()\n",
    "    for value in values:\n",
    "        if value == 'amin':\n",
    "            d[value] = var[value].min().to_dict()\n",
    "        elif value == 'amax':\n",
    "            d[value] = var[value].max().to_dict()\n",
    "        else:\n",
    "            d[value] = var.apply(lambda x: calculate_average_weight(x=x, value=value, weights=weights)).to_dict()\n",
    "    if 'p50' in values and 'p25' in values and 'p75' in values:\n",
    "        d['p75_max'] = var['p75'].max().to_dict()\n",
    "        d['p25_min'] = var['p25'].min().to_dict()\n",
    "    d[weights] = var.apply(lambda x: x[weights].sum()).to_dict()\n",
    "    return pd.DataFrame(d).transpose().to_dict()\n",
    "\n",
    "def calculate_average_weight(x: pd.DataFrame,\n",
    "                             value: str,\n",
    "                             weights: str) -> float:\n",
    "    return np.average(x[value], weights=x[weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_groups = create_reference_groups(data=mappings.loc[mappings.p50.notna() & (mappings.num_records > 100) & (mappings.p25 != mappings.p75)],\n",
    "                            group_by=TARGET,\n",
    "                            filter_by=HOSPITAL_COLUMN,\n",
    "                            filter_on=mappings[HOSPITAL_COLUMN].unique(), #TRAIN_HOSPITALS,\n",
    "                            values=DATA_DISTRIBUTION_COLUMNS,\n",
    "                            weights=DATA_DISTRIBUTION_WEIGHTS,\n",
    "                            )\n",
    "# Remove unmapped from reference groups as it is not a numeric concept\n",
    "reference_groups.pop('unmapped')\n",
    "\n",
    "parameter_groups = create_reference_groups(data=mappings,\n",
    "                                           group_by='id',\n",
    "                                           filter_by=HOSPITAL_COLUMN,\n",
    "                                           filter_on=[x for x in mappings[HOSPITAL_COLUMN] if x not in TRAIN_HOSPITALS],\n",
    "                                           values=DATA_DISTRIBUTION_COLUMNS,\n",
    "                                           weights=DATA_DISTRIBUTION_WEIGHTS,\n",
    "                                           )\n",
    "\n",
    "df_reference_groups = pd.DataFrame(reference_groups).transpose()\n",
    "os.makedirs('./output/similarity', exist_ok=True)\n",
    "df_reference_groups.to_csv('./output/similarity/reference_groups.csv')\n",
    "print(df_reference_groups.info())\n",
    "df_reference_groups.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_groups.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get base predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "AutoMap class\n",
    "\"\"\"\n",
    "\n",
    "# import\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "class AutoMap:\n",
    "    \"\"\"\n",
    "    :params:\n",
    "    train_files: list of files to load for training the prediction pipeline on\n",
    "    predict_files: list of files to read and predict the parameters for\n",
    "    output_files: list of files to save the predictions to, where length equals length of predict_files\n",
    "    pipe_file: Previously created compatible pickled tuple of fitted (Pipeline, LabelEncoder) objects\n",
    "\n",
    "\n",
    "    :args:\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')  # hidden in Pipe creation\n",
    "\n",
    "        # column names to use in training/predicting\n",
    "        self.source = 'parameter_name'\n",
    "        self.target = 'pacmed_subname'\n",
    "        self.pred = 'predicted_subname'\n",
    "\n",
    "        # if no label is given, impute with index[0] (unmapped)\n",
    "        self.unlabeled = ['unmapped', 'microbiology']  # used for validation to filter out unlabeled\n",
    "\n",
    "        # initial r'\\w+' but 5% performance gain when underscores are omitted\n",
    "        self.preprocess_text_regex_expression = r'[a-zA-Z0-9]+'\n",
    "        return\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Tokenise words while ignoring punctuation\n",
    "        tokeniser = RegexpTokenizer(self.preprocess_text_regex_expression)\n",
    "        tokens = tokeniser.tokenize(text)\n",
    "\n",
    "        # Lowercase and lemmatise\n",
    "        lemmatiser = WordNetLemmatizer()\n",
    "        lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "\n",
    "        # Remove stop words\n",
    "        # keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "        # return keywords\n",
    "        return lemmas\n",
    "\n",
    "    def create_pipe(self,\n",
    "                    X=pd.Series,\n",
    "                    y=pd.Series,\n",
    "                    estimator=SGDClassifier(random_state=123),\n",
    "                    grid: dict = None,\n",
    "                    cv: int = 10,\n",
    "                    n_jobs: int = None,\n",
    "                    save: bool = False,\n",
    "                    prefix=None):\n",
    "        \"\"\"\n",
    "        Create the pipe object used to train and test text data\n",
    "        \"\"\"\n",
    "\n",
    "        if y.isna().sum() > 0:\n",
    "            y = y.fillna(self.unlabeled[0])\n",
    "\n",
    "        # ensure labels are encoded\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(y=y.unique())\n",
    "\n",
    "        # Create an instance of TfidfVectorizer\n",
    "        vectoriser = TfidfVectorizer(analyzer=self.preprocess_text)\n",
    "\n",
    "        # Fit to the data and transform to feature matrix\n",
    "        X_train_tfidf = vectoriser.fit_transform(X)\n",
    "\n",
    "        # try an initial accuracy before hyperparameter optimization\n",
    "        clf = estimator\n",
    "        # clf = SGDClassifier(random_state=123)\n",
    "        # clf_scores = cross_val_score(clf, X_train_tfidf, self.y_train, cv=10)\n",
    "        # print(clf_scores)\n",
    "        # print(\"SGDClassfier Accuracy: %0.2f (+/- %0.2f)\" % (clf_scores.mean(), clf_scores.std() * 2))\n",
    "\n",
    "        if grid is None:\n",
    "            grid = {'fit_intercept': [True, False],\n",
    "                    'early_stopping': [True, False],\n",
    "                    'loss': ['log', 'modified_huber', 'perceptron', 'huber', 'squared_loss', 'epsilon_insensitive',\n",
    "                             'squared_epsilon_insensitive'],\n",
    "                    # ['hinge', 'log', 'squared_hinge'], #PM squared_loss --> squared_error in v1.2\n",
    "                    'penalty': ['l2', 'l1', 'none']}\n",
    "\n",
    "            # Reduce to optimal grid for rerunning code\n",
    "            grid = {'fit_intercept': [True],\n",
    "                    'early_stopping': [False],\n",
    "                    'loss': ['modified_huber'],\n",
    "                    'penalty': ['l2']}\n",
    "\n",
    "        # retry the SGDClassifier training with param_grid\n",
    "        search = GridSearchCV(estimator=clf, param_grid=grid, cv=cv, n_jobs=n_jobs)\n",
    "        search.fit(X_train_tfidf, y)\n",
    "\n",
    "        # grid_sgd_clf_scores = cross_val_score(search.best_estimator_, X_train_tfidf, self.y_train, cv=5)\n",
    "        # print(grid_sgd_clf_scores)\n",
    "        # print(\"SGDClassifier optimal grid Accuracy: %0.2f (+/- %0.2f)\" % (\n",
    "        # grid_sgd_clf_scores.mean(), grid_sgd_clf_scores.std() * 2))\n",
    "\n",
    "        # create Pipeline with vectoriser and optimal classifier\n",
    "        self.pipe = Pipeline([('vectoriser', vectoriser),\n",
    "                              ('classifier', search)])  # clf\n",
    "\n",
    "        # fit the pipeline to the full training data\n",
    "        self.pipe.fit(X, self.le.transform(y.values))\n",
    "\n",
    "        # save pipe to file to prevent rerunning the same pipelines\n",
    "        if prefix is None:\n",
    "            prefix = ''\n",
    "        if save:\n",
    "            f_name = f'./data/pipes/{prefix}__{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.pipe'\n",
    "            joblib.dump((self.pipe,\n",
    "                         self.le,\n",
    "                         ),\n",
    "                        f_name,\n",
    "                        compress=('gzip', 3),\n",
    "                        protocol=5)\n",
    "            print(f\"Pipeline saved to: {f_name}\")\n",
    "\n",
    "        return self.pipe\n",
    "\n",
    "    def save_pipe(self, f_name):\n",
    "        joblib.dump((self.pipe, self.le), f_name)\n",
    "        print(f\"Pipeline saved to: {f_name}\")\n",
    "\n",
    "    def load_pipe(self, f_name):\n",
    "        if os.path.isfile(f_name):\n",
    "            self.pipe, self.le = joblib.load(f_name)\n",
    "        else:\n",
    "            self.pipe = None\n",
    "            self.le = LabelEncoder()\n",
    "        print(f\"Pipeline loaded from: {f_name}\")\n",
    "\n",
    "    def predict_proba_transformed(self, X, **predict_proba_params):\n",
    "\n",
    "        if isinstance(X, pd.Series):\n",
    "            probs = self.pipe.predict_proba(X, **predict_proba_params)\n",
    "            id_vars = [X.name]\n",
    "            X = pd.DataFrame(X)\n",
    "        else:\n",
    "            probs = self.pipe.predict_proba(X[X.columns[1]], **predict_proba_params)\n",
    "            id_vars = list(X.columns)\n",
    "            print(id_vars)\n",
    "\n",
    "        c = pd.concat(\n",
    "            [X.reset_index(drop=True),\n",
    "             pd.DataFrame(probs, columns=self.le.classes_),\n",
    "             ],\n",
    "            axis=1)\n",
    "        c.loc[:, self.le.classes_] = c.loc[:, self.le.classes_].replace(0, np.nan)\n",
    "        return (c\n",
    "            .set_index(id_vars)\n",
    "            .stack()\n",
    "            .reset_index()\n",
    "            .rename(columns={\n",
    "                \"level_1\": \"label\",\n",
    "                \"level_2\": \"label\",\n",
    "                0: \"value\",\n",
    "            }\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('./output/base/', ignore_errors=True)\n",
    "\n",
    "os.makedirs('./output/base/overlapping', exist_ok=True)\n",
    "os.makedirs('./output/base/non_overlapping', exist_ok=True)\n",
    "os.makedirs('./output/similarity', exist_ok=True)\n",
    "os.makedirs('./output/skewness', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoMap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m am \u001B[38;5;241m=\u001B[39m \u001B[43mAutoMap\u001B[49m()\n\u001B[0;32m      3\u001B[0m train_data \u001B[38;5;241m=\u001B[39m mappings\u001B[38;5;241m.\u001B[39mloc[mappings[HOSPITAL_COLUMN]\u001B[38;5;241m.\u001B[39misin(TRAIN_HOSPITALS)]\n\u001B[0;32m      4\u001B[0m test_data_overlap \u001B[38;5;241m=\u001B[39m mappings\u001B[38;5;241m.\u001B[39mloc[\u001B[38;5;241m~\u001B[39mmappings\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39misin(train_data\u001B[38;5;241m.\u001B[39mindex)]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'AutoMap' is not defined"
     ]
    }
   ],
   "source": [
    "am = AutoMap()\n",
    "\n",
    "train_data = mappings.loc[mappings[HOSPITAL_COLUMN].isin(TRAIN_HOSPITALS)]\n",
    "test_data_overlap = mappings.loc[~mappings.index.isin(train_data.index)]\n",
    "test_data_non_overlap = mappings.loc[~mappings.index.isin(train_data.index) & ~mappings[SOURCE].isin(train_data[SOURCE].unique())]\n",
    "\n",
    "\n",
    "am.create_pipe(X=train_data[SOURCE],\n",
    "               y=train_data[TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = am.pipe.steps[0][1].transform(test_data_overlap[['parameter_name']])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_overlap[['parameter_name']].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = am.pipe.steps[0][1].transform(test_data_overlap[['parameter_name']].iloc[1])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.pipe.steps[0][1].transform(test_data_overlap[['concept_label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data_non_overlap\n",
    "predicted_labels = am.predict_proba_transformed(test_data[['id', SOURCE]]).sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels[TARGET + '_original'] = predicted_labels.id.map(test_data.set_index('id')[TARGET].to_dict())\n",
    "\n",
    "save_loc='./output/base/non_overlapping/'\n",
    "\n",
    "predicted_labels.to_csv(f'{save_loc}predicted_labels.csv')\n",
    "test_data.to_csv(f'{save_loc}test_data.csv')\n",
    "train_data.to_csv(f'{save_loc}train_data.csv')\n",
    "joblib.dump(am, f'{save_loc}am.joblib')\n",
    "\n",
    "results = dict()\n",
    "for i in range(1, 11):\n",
    "    results[i] = calculate_scores(predicted_labels, test_data[['id', 'hospital_name' , 'ehr_name', 'concept_label', 'num_records']], {TARGET: concept_category_groups}, rank=i)\n",
    "    for key, value in results[i].items():\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            for col in value.columns:\n",
    "                if value[col].dtype == 'float64':\n",
    "                    if (value[col] == value[col].astype(int).astype(float)).all():\n",
    "                        value[col] = value[col].astype(int)\n",
    "            value.to_csv(f'{save_loc}aprf__rank_{i}__{key}.csv')\n",
    "            value.round(3).to_csv(f'{save_loc}aprf__rank_{i}__{key}__round3.csv', float_format='%.3f')\n",
    "\n",
    "plot_results(results, score_type='precision', save_loc=save_loc)\n",
    "plot_results(results, score_type='recall', save_loc=save_loc)\n",
    "plot_results(results, score_type='f1', save_loc=save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base performance on overlapping data\n",
    "test_data = test_data_overlap\n",
    "predicted_labels = am.predict_proba_transformed(test_data[['id', SOURCE]]).sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels[TARGET + '_original'] = predicted_labels.id.map(test_data.set_index('id')[TARGET].to_dict())\n",
    "\n",
    "save_loc = './output/base/overlapping/'\n",
    "predicted_labels.to_csv(f'{save_loc}predicted_labels.csv')\n",
    "test_data.to_csv(f'{save_loc}test_data.csv')\n",
    "\n",
    "results = dict()\n",
    "for i in range(1, 11):\n",
    "    results[i] = calculate_scores(predicted_labels, test_data[['id', 'hospital_name' , 'ehr_name', 'concept_label', 'num_records']], {TARGET: concept_category_groups}, rank=i)\n",
    "    for key, value in results[i].items():\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            for col in value.columns:\n",
    "                if value[col].dtype == 'float64':\n",
    "                    if (value[col] == value[col].astype(int).astype(float)).all():\n",
    "                        value[col] = value[col].astype(int)\n",
    "            value.to_csv(f'{save_loc}aprf__rank_{i}__{key}.csv')\n",
    "            value.round(3).to_csv(f'{save_loc}aprf__rank_{i}__{key}__round3.csv', float_format='%.3f')\n",
    "            if (key == 'relevance') & (i == 1):\n",
    "                print(value.round(3))\n",
    "\n",
    "plot_results(results, score_type='precision', save_loc=save_loc)\n",
    "plot_results(results, score_type='recall', save_loc=save_loc)\n",
    "plot_results(results, score_type='f1', save_loc=save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "Base performance for the parameter name only model is quite acceptable, with an overall accuracy of 77% split over an accuracy of 92% for irrelevant parameters (1 class) and 63% for identifying the correct label for relevant parameters which contain >1000 classes.\n",
    "\n",
    "The second predicted label (rank 1) shows an increase of 5%, 9% and 14% respectively, indicating an improvement in the model is possible by helping to filter out incorrect labels at the first position. The third and further predicted labels show a steep decline in predictive power and therefore are less relevant to improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity and Skewness\n",
    "Similarity and skewness will only be evaluated for numeric parameters as non-numeric parameters will not show the same distribution. However, non-numeric parameters may still have been mapped to numeric concepts as medication is frequently registered as '1' where the dosage is noted in the title (e.g. 1000mg paracetamol). Therefore, we will only focus on numeric parameters and numeric concepts, while allowing non-numeric parameters to receive any prediction, and numeric parameters to only receive numeric predictions or unmapped.\n",
    "\n",
    "We defined numeric parameters as:\n",
    "- At least 100 registrations\n",
    "- Availability of the p50 column\n",
    "- At least a difference between the p25 and p75 columns (remove parameters with only '1' or all the same values)\n",
    "- Not a minimum of 0 with a maximum of 1 (remove parameters with only '0' or '1' values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_num = test_data.loc[\n",
    "    (test_data.num_records > 100) &\n",
    "    (test_data.p50.notna()) &\n",
    "    (test_data.p75 != test_data.p25) &\n",
    "    (~((test_data.amin == 0) & (test_data.amax == 1)))\n",
    "]\n",
    "print(test_data_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base performance on overlapping data of numerical parameters\n",
    "test_data = test_data_num\n",
    "predicted_labels = am.predict_proba_transformed(test_data[['id', SOURCE]]).sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels[TARGET + '_original'] = predicted_labels.id.map(test_data.set_index('id')[TARGET].to_dict())\n",
    "\n",
    "save_loc = './output/base/numeric/'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "predicted_labels.to_csv(f'{save_loc}predicted_labels.csv')\n",
    "test_data.to_csv(f'{save_loc}test_data.csv')\n",
    "\n",
    "results = dict()\n",
    "for i in range(1, 11):\n",
    "    results[i] = calculate_scores(predicted_labels, test_data[['id', 'hospital_name' , 'ehr_name', 'concept_label', 'num_records']], {TARGET: concept_category_groups}, rank=i)\n",
    "    for key, value in results[i].items():\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            for col in value.columns:\n",
    "                if value[col].dtype == 'float64':\n",
    "                    if (value[col] == value[col].astype(int).astype(float)).all():\n",
    "                        value[col] = value[col].astype(int)\n",
    "            value.to_csv(f'{save_loc}aprf__rank_{i}__{key}.csv')\n",
    "            value.round(3).to_csv(f'{save_loc}aprf__rank_{i}__{key}__round3.csv', float_format='%.3f')\n",
    "            if (key == 'relevance') & (i == 1):\n",
    "                print(value.round(3))\n",
    "\n",
    "plot_results(results, score_type='precision', save_loc=save_loc)\n",
    "plot_results(results, score_type='recall', save_loc=save_loc)\n",
    "plot_results(results, score_type='f1', save_loc=save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_groups.keys().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base performance on overlapping data of numerical parameters, when dropping all non-numeric concepts, except for unmapped\n",
    "test_data = test_data_num\n",
    "predicted_labels = am.predict_proba_transformed(test_data[['id', SOURCE]]).sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels[TARGET + '_original'] = predicted_labels.id.map(test_data.set_index('id')[TARGET].to_dict())\n",
    "print(predicted_labels.shape)\n",
    "predicted_labels = predicted_labels.loc[predicted_labels['label'].isin(list(reference_groups.keys()) + ['unmapped'])].copy()\n",
    "print(predicted_labels.shape)\n",
    "\n",
    "save_loc = './output/base/numeric/drop_non_numeric_concepts/'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "predicted_labels.to_csv(f'{save_loc}predicted_labels.csv')\n",
    "test_data.to_csv(f'{save_loc}test_data.csv')\n",
    "\n",
    "results = dict()\n",
    "for i in range(1, 11):\n",
    "    results[i] = calculate_scores(predicted_labels, test_data[['id', 'hospital_name' , 'ehr_name', 'concept_label', 'num_records']], {TARGET: concept_category_groups}, rank=i)\n",
    "    for key, value in results[i].items():\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            for col in value.columns:\n",
    "                if value[col].dtype == 'float64':\n",
    "                    if (value[col] == value[col].astype(int).astype(float)).all():\n",
    "                        value[col] = value[col].astype(int)\n",
    "            value.to_csv(f'{save_loc}aprf__rank_{i}__{key}.csv')\n",
    "            value.round(3).to_csv(f'{save_loc}aprf__rank_{i}__{key}__round3.csv', float_format='%.3f')\n",
    "            if (key == 'relevance') & (i == 1):\n",
    "                print(value.round(3))\n",
    "\n",
    "plot_results(results, score_type='precision', save_loc=save_loc)\n",
    "plot_results(results, score_type='recall', save_loc=save_loc)\n",
    "plot_results(results, score_type='f1', save_loc=save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_statistic(data_1: Dict[str, float], data_2: Dict[str, float]\n",
    "                         ) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a modified similarity statistic based on the T-statistic where higher values signal less similarity.\n",
    "    A similarity of 1 describes exact matching medians, values below 1 describe a variance within the group and/or parameter large enough for the lower term to be larger than the absolute difference in medians. Values above 1 describe variance too low to compensate for the difference in median. Therefore, an arbitrary value of the statistic can be chosen for conversion to boolean.\n",
    "    :param data_1: Dictionary with keys 'p50', 'p25', 'p75', 'num_records'\n",
    "    :param data_2: Dictionary with keys 'p50', 'p25', 'p75', 'num_records'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        median_1 = data_1['p50']\n",
    "        iqr_1 = data_1['p75'] - data_1['p25']\n",
    "        num_1 = data_1['num_records']\n",
    "        median_2 = data_2['p50']\n",
    "        iqr_2 = data_2['p75'] - data_2['p25']\n",
    "        num_2 = data_2['num_records']\n",
    "\n",
    "        if num_1 < 1: #10_000:\n",
    "             return np.nan\n",
    "\n",
    "        med_abs = abs(median_1 - median_2)\n",
    "\n",
    "        j = dict()\n",
    "        for i, (iqr, num) in enumerate(zip([iqr_1, iqr_2], [num_1, num_2])):\n",
    "            j[i] = iqr ** 2 # / (1.82 * 1) # num)\n",
    "\n",
    "        lower_term =  (j[0] + j[1]) ** 0.5 # * (np.pi / 2) ** 0.5\n",
    "        if lower_term == 0:\n",
    "            return np.nan\n",
    "        return med_abs / lower_term\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def get_similarity(x: pd.DataFrame,\n",
    "                   parameter_groups: Dict[int, Dict[str, float]],\n",
    "                   reference_groups: Dict[str, Dict[str, float]],\n",
    "                  ) -> float:\n",
    "    try:\n",
    "        return similarity_statistic(parameter_groups.get(x['id'], dict()),\n",
    "                                    reference_groups.get(x['label'], dict()))\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(score_values: Dict[int, float],\n",
    "                    base_values: Dict[int, float],\n",
    "                    count_values: Dict[int, int],\n",
    "                    base_performance: float = 0.5997,\n",
    "                    color_palette=None,\n",
    "                    xrange: Tuple[float, float] = (0, 20),\n",
    "                    yrange: Tuple[float, float] = (0, 2000),\n",
    "                    analysis_title: str = None,\n",
    "                    save_loc=None,\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Plots the results of the evaluation.\n",
    "    :param score_values:\n",
    "    :param count_values:\n",
    "    :param color_palette:\n",
    "    :param save_loc:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 1})\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1,\n",
    "                                   sharex=True,\n",
    "                                   figsize=(6,6),\n",
    "                                   gridspec_kw={'height_ratios': [6,2],\n",
    "                                                },\n",
    "                                   )\n",
    "\n",
    "    # Plot scores\n",
    "    sns.lineplot(x=score_values.keys(),\n",
    "                 y=score_values.values(),\n",
    "                 color='black', linestyle='solid',\n",
    "                 legend=True,\n",
    "                 label=f\"{analysis_title} adjusted\",\n",
    "                 ax=ax1,\n",
    "                 )\n",
    "    sns.lineplot(x=base_values.keys(),\n",
    "                 y=base_values.values(),\n",
    "                 color='black', linestyle='dashed',\n",
    "                 legend=True,\n",
    "                 label='Baseline affected parameters',\n",
    "                 ax=ax1,\n",
    "                 )\n",
    "    sns.lineplot(x=score_values.keys(), y=[base_performance]*len(score_values.keys()), ax=ax1, color='black', linestyle='dotted', legend=False, label='Baseline numeric parameters')\n",
    "    ax1.set_xlabel('Rank of predicted labels')\n",
    "    ax1.set_ylabel(f\"Recall\".capitalize(), labelpad=25)\n",
    "    ylim_min = min([min(score_values.values()), min(base_values.values()), base_performance]) / 1.01\n",
    "    ylim_max = max([max(score_values.values()), max(base_values.values()), base_performance]) * 1.01\n",
    "\n",
    "    ax1.set_ylim(0,1)  # ylim_min, ylim_max)\n",
    "    ax1.set_xlim(*xrange)  # max(score_values.keys()))\n",
    "    ax1.legend(\n",
    "        loc='upper right',\n",
    "        # bbox_to_anchor=(1.0, 0.0),\n",
    "        ncol=1,\n",
    "    )\n",
    "\n",
    "    # Plot parameter counts\n",
    "    sns.lineplot(x=count_values.keys(),\n",
    "                 y=count_values.values(),\n",
    "                 color='black', linestyle='solid',\n",
    "                 legend=False,\n",
    "                 ax=ax2,\n",
    "                 )\n",
    "    ax2.set_ylim(*yrange) # 0, 510)  # max(count_values.values())*1.05)\n",
    "    ax2.set_xlim(*xrange)  # max(count_values.keys()))\n",
    "    ax2.set_xlabel(f\"{analysis_title} thresholds\")\n",
    "    ax2.set_ylabel('Parameter\\ncount')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    source_data = pd.DataFrame([score_values, base_values, count_values],\n",
    "                 index=[analysis_title, 'baseline_affected', 'parameter_count']).transpose()\n",
    "    source_data['baseline_all'] = base_performance\n",
    "    source_data.index.name = f'{analysis_title}_threshold'\n",
    "    source_data.to_csv(f\"{save_loc}plot__{analysis_title}__recall.csv\")\n",
    "    print(source_data.head(5))\n",
    "    print(source_data.loc[source_data[analysis_title] > source_data['baseline_affected']].head(20))\n",
    "    plt.savefig(f\"{save_loc}plot__{analysis_title}__recall.png\", dpi=1200)\n",
    "    plt.savefig(f\"{save_loc}plot__{analysis_title}__recall.pdf\", dpi=1200)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data_num\n",
    "predicted_labels = am.predict_proba_transformed(test_data[['id', SOURCE]]).sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels[TARGET + '_original'] = predicted_labels.id.map(test_data.set_index('id')[TARGET].to_dict())\n",
    "predicted_labels = predicted_labels.sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels['rank'] = predicted_labels.groupby('id').cumcount() + 1\n",
    "predicted_labels['correct'] = predicted_labels['label'] == predicted_labels['concept_label_original']\n",
    "\n",
    "parameter_groups = test_data.set_index('id').transpose().to_dict()\n",
    "\n",
    "# calculate similarity for all predictions\n",
    "predicted_labels['similarity'] = predicted_labels.apply(lambda x: get_similarity(x, parameter_groups, reference_groups), axis=1)\n",
    "print(f\"Filtering parameters on numeric criteria results in {test_data_num.shape[0]} parameters of which {(test_data_num.concept_label != 'unmapped').sum() / test_data_num.shape[0]:.2%} was labeled as relevant. A similarity was calculated for {predicted_labels.similarity.notna().sum()} out of {predicted_labels.shape[0]} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_num.concept_label.map(concept_category_groups).value_counts(ascending=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels['similarity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Similarity adjustment - Only numeric parameters and numeric concepts adjusted (keeps all unmapped and all non-numeric concepts untouched)\n",
    "\n",
    "save_loc = './output/base/numeric/similarity/keep_non_numeric_concepts/'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "\n",
    "similarity_threshold = 1000\n",
    "sim_results = dict()\n",
    "sim_results_counts = dict()\n",
    "sim_results_base = dict()\n",
    "\n",
    "for j in range(0, similarity_threshold, 1):\n",
    "    i = j / 10\n",
    "    predicted_labels['rank_similarity'] = predicted_labels.groupby(['id', ((predicted_labels['similarity'] < i) | predicted_labels['similarity'].isna())]).cumcount() + 1\n",
    "    predicted_labels['rank_similarity'] = predicted_labels['rank_similarity'].where((predicted_labels['similarity'] < i) | predicted_labels['similarity'].isna(), np.nan)\n",
    "\n",
    "    affected_parameters = predicted_labels.loc[(predicted_labels['rank'] == 1) & (predicted_labels['rank_similarity'] != 1)]['id'].unique()\n",
    "    if len(affected_parameters) <= 1:\n",
    "        break\n",
    "    pl = predicted_labels.loc[predicted_labels['rank_similarity'] == 1]\n",
    "    sim_results[i] = predicted_labels.loc[predicted_labels['id'].isin(affected_parameters) & (predicted_labels['rank_similarity'] == 1)]['correct'].mean()\n",
    "    sim_results_base[i] = predicted_labels.loc[predicted_labels['id'].isin(affected_parameters)].drop_duplicates(subset=['id'])['correct'].mean()\n",
    "    sim_results_counts[i] = len(affected_parameters)\n",
    "\n",
    "plot_similarity(score_values=sim_results,\n",
    "                count_values=sim_results_counts,\n",
    "                base_values=sim_results_base,\n",
    "                save_loc=save_loc,\n",
    "                xrange=(0, 10),\n",
    "                yrange=(0, 3001),\n",
    "                analysis_title=\"Similarity\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Similarity adjustment - Numeric concepts are adjusted, non-numeric concepts are dropped except for unmapped\n",
    "\n",
    "save_loc = './output/base/numeric/similarity/drop_non_numeric_concepts/'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "\n",
    "similarity_threshold = 1000\n",
    "sim_results = dict()\n",
    "sim_results_counts = dict()\n",
    "sim_results_base = dict()\n",
    "\n",
    "for j in range(0, similarity_threshold, 1):\n",
    "    i = j / 10\n",
    "    predicted_labels['rank_similarity'] = predicted_labels.groupby(['id', ((predicted_labels['similarity'] < i) | (predicted_labels['similarity'].isna() & (predicted_labels['label'] == 'unmapped')))]).cumcount() + 1\n",
    "    predicted_labels['rank_similarity'] = predicted_labels['rank_similarity'].where((predicted_labels['similarity'] < i) | (predicted_labels['similarity'].isna() & (predicted_labels['label'] == 'unmapped')), np.nan)\n",
    "\n",
    "    unique_parameters = predicted_labels.loc[predicted_labels['rank'] != predicted_labels['rank_similarity']]['id'].unique()\n",
    "    affected_parameters = predicted_labels.loc[(predicted_labels['rank'] == 1) & (predicted_labels['rank_similarity'] != 1)]['id'].unique()\n",
    "    if len(affected_parameters) <= 1:\n",
    "        break\n",
    "    pl = predicted_labels.loc[predicted_labels['rank_similarity'] == 1]\n",
    "    sim_results[i] = predicted_labels.loc[predicted_labels['id'].isin(affected_parameters) & (predicted_labels['rank_similarity'] == 1)]['correct'].mean()\n",
    "    sim_results_base[i] = predicted_labels.loc[predicted_labels['id'].isin(affected_parameters)].drop_duplicates(subset=['id'])['correct'].mean()\n",
    "    sim_results_counts[i] = len(affected_parameters)\n",
    "\n",
    "plot_similarity(score_values=sim_results,\n",
    "                count_values=sim_results_counts,\n",
    "                base_values=sim_results_base,\n",
    "                save_loc=save_loc,\n",
    "                xrange=(0, 10),\n",
    "                yrange=(0, 3001),\n",
    "                analysis_title=\"Similarity\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Similarity adjustment - Numeric parameters evaluated, without adjusting for similarity, but non-numeric concepts are dropped except for unmapped.\n",
    "test_data = test_data_num.copy()\n",
    "predicted_labels = am.predict_proba_transformed(test_data[['id', SOURCE]]).sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels[TARGET + '_original'] = predicted_labels.id.map(test_data.set_index('id')[TARGET].to_dict())\n",
    "\n",
    "save_loc = './output/base/numeric/similarity/drop_non_numeric_concepts/drop_only_non_numeric_without_adjusting_numerics/'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "predicted_labels.to_csv(f'{save_loc}predicted_labels.csv')\n",
    "test_data.to_csv(f'{save_loc}test_data.csv')\n",
    "\n",
    "predicted_labels = predicted_labels.sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels['rank'] = predicted_labels.groupby('id').cumcount() + 1\n",
    "predicted_labels['correct'] = predicted_labels['label'] == predicted_labels['concept_label_original']\n",
    "\n",
    "parameter_groups = test_data.set_index('id').transpose().to_dict()\n",
    "\n",
    "\n",
    "# get similarity for numeric parameters\n",
    "predicted_labels['similarity'] = predicted_labels.apply(lambda x: get_similarity(x, parameter_groups, reference_groups), axis=1)\n",
    "\n",
    "# for numeric items, adjust predictions\n",
    "predicted_labels = predicted_labels.loc[predicted_labels['similarity'].notna() | (predicted_labels['label'] == 'unmapped')].copy()\n",
    "\n",
    "results = dict()\n",
    "for i in range(1, 11):\n",
    "    results[i] = calculate_scores(predicted_labels, test_data[['id', 'hospital_name' , 'ehr_name', 'concept_label', 'num_records']], {TARGET: concept_category_groups}, rank=i)\n",
    "    for key, value in results[i].items():\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            for col in value.columns:\n",
    "                if value[col].dtype == 'float64':\n",
    "                    if (value[col] == value[col].astype(int).astype(float)).all():\n",
    "                        value[col] = value[col].astype(int)\n",
    "            value.to_csv(f'{save_loc}aprf__rank_{i}__{key}.csv')\n",
    "            value.round(3).to_csv(f'{save_loc}aprf__rank_{i}__{key}__round3.csv', float_format='%.3f')\n",
    "            if (key == 'relevance') & (i == 1):\n",
    "                print(value.round(3))\n",
    "\n",
    "plot_results(results, score_type='precision', save_loc=save_loc)\n",
    "plot_results(results, score_type='recall', save_loc=save_loc)\n",
    "plot_results(results, score_type='f1', save_loc=save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Similarity adjustment - All parameters evaluated, but only numeric concepts adjusted, non-numeric concepts are dropped except for unmapped.\n",
    "\n",
    "test_data = test_data_overlap.copy()\n",
    "predicted_labels = am.predict_proba_transformed(test_data[['id', SOURCE]]).sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels[TARGET + '_original'] = predicted_labels.id.map(test_data.set_index('id')[TARGET].to_dict())\n",
    "\n",
    "save_loc = './output/base/overlapping/similarity/drop_non_numeric_concepts/'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "predicted_labels.to_csv(f'{save_loc}predicted_labels.csv')\n",
    "test_data.to_csv(f'{save_loc}test_data.csv')\n",
    "\n",
    "predicted_labels = predicted_labels.sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels['rank'] = predicted_labels.groupby('id').cumcount() + 1\n",
    "predicted_labels['correct'] = predicted_labels['label'] == predicted_labels['concept_label_original']\n",
    "\n",
    "parameter_groups = test_data.set_index('id').transpose().to_dict()\n",
    "\n",
    "numeric_parameters = test_data.loc[\n",
    "    (test_data.num_records > 100) &\n",
    "    (test_data.p50.notna()) &\n",
    "    (test_data.p75 != test_data.p25) &\n",
    "    (~((test_data.amin == 0) & (test_data.amax == 1)))\n",
    "]['id'].unique()\n",
    "\n",
    "numeric_parameters_dict = {i: True for i in numeric_parameters}\n",
    "predicted_labels['is_numeric'] = predicted_labels['id'].map(numeric_parameters_dict).fillna(False)\n",
    "\n",
    "# get similarity for numeric parameters\n",
    "predicted_labels['similarity'] = predicted_labels.apply(lambda x: get_similarity(x, parameter_groups, reference_groups), axis=1)\n",
    "\n",
    "# for numeric items, adjust predictions\n",
    "similarity_threshold = 5\n",
    "\n",
    "predicted_labels = predicted_labels.loc[(~predicted_labels['is_numeric']) | (predicted_labels['is_numeric'] & ((predicted_labels['similarity'] < similarity_threshold) | predicted_labels['label'] == 'unmapped'))].copy()\n",
    "\n",
    "results = dict()\n",
    "for i in range(1, 11):\n",
    "    results[i] = calculate_scores(predicted_labels, test_data[['id', 'hospital_name' , 'ehr_name', 'concept_label', 'num_records']], {TARGET: concept_category_groups}, rank=i)\n",
    "    for key, value in results[i].items():\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            for col in value.columns:\n",
    "                if value[col].dtype == 'float64':\n",
    "                    if (value[col] == value[col].astype(int).astype(float)).all():\n",
    "                        value[col] = value[col].astype(int)\n",
    "            value.to_csv(f'{save_loc}aprf__rank_{i}__{key}.csv')\n",
    "            value.round(3).to_csv(f'{save_loc}aprf__rank_{i}__{key}__round3.csv', float_format='%.3f')\n",
    "            if (key == 'relevance') & (i == 1):\n",
    "                print(value.round(3))\n",
    "\n",
    "plot_results(results, score_type='precision', save_loc=save_loc)\n",
    "plot_results(results, score_type='recall', save_loc=save_loc)\n",
    "plot_results(results, score_type='f1', save_loc=save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "Parameters with low frequency counts are not an evident source of noise for label prediction based on parameter names. Most notably, parameters with a high record count actually show a slightly more noisy prediction as the jump from the first label to second label is larger than for the complete dataset.\n",
    "\n",
    "This may indicate that sorting of labels may be most relevant for parameters with high number of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare performance stratified by numeric/non-numeric data and record counts\n",
    "We want to identify potential targets for improvement of predicted parameters. One of these targets, is the use of underlying data distributions based on the p25, p50, p75 and number of records. Therefore, these attempts will only be useful for numeric items with at least enough data to have a representative measurement of these values.\n",
    "\n",
    "We will therefore compare performance on numeric vs. non-numeric data, as well as compare the performance for parameters which are numeric (p50), have at least 10 records and which have different values for p25 and p75. We also check the performance for the records which will remain (either no p50, less than 10 records, or matching p25 and p75)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare numeric vs non-numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Conclusion\n",
    "Data fit for data distribution analysis constitutes 9631 parameters, leaving 49173 parameters unfit. Their potential for improvement in score at the first position, when considering the correctness of the second label is 7.2% and 17% for irrelevant and relevant parameters respectively for the 10k numeric parameters, and 4.6% and 12.7% for the 49k non-numeric parameters.\n",
    "\n",
    "The potential for overall improvement is therefore limited to circa 3% on the entire full dataset (10k * 17% => 1700 / 59k parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "We will continue working only with records labeled as numeric items.\n",
    "\n",
    "1. Only allow to predict concepts which have numeric values, but this is dependent on the training hospitals having parameters with numerics in these concepts. A drop in performance may therefore be caused by having too little training data.\n",
    "\n",
    "2. Calculate the similarity between the parameter and the reference groups for the predicted concepts. This would result in a new value describing the difference in various terms: similarity statistic which calculates the difference in medians in comparison to their IQRs, and the p50/IQR or p_skewedness ((p50-p25)/iqr), which describes the data distribution: does the data have a wide range or a narrow range surrounding the median, and its inverse. By normalizing the p50 for the p25 and p75, we can describe skewedness in data: medians close to p25 will have values <0.5 and the other way around.\n",
    "\n",
    "3. Calculate similarity, but only evaluate for concept labels which are present in the reference groups and only adjust those predictions if the records are too different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data_num\n",
    "predicted_labels = am.predict_proba_transformed(test_data[['id', SOURCE]]).sort_values(['id', 'value'], ascending=[True, False])\n",
    "\n",
    "predicted_labels[TARGET + '_original'] = predicted_labels.id.map(test_data.set_index('id')[TARGET].to_dict())\n",
    "predicted_labels = predicted_labels.sort_values(['id', 'value'], ascending=[True, False])\n",
    "predicted_labels['rank'] = predicted_labels.groupby('id').cumcount() + 1\n",
    "predicted_labels['correct'] = predicted_labels['label'] == predicted_labels['concept_label_original']\n",
    "\n",
    "parameter_groups = test_data.set_index('id').transpose().to_dict()\n",
    "\n",
    "# calculate similarity for all predictions\n",
    "predicted_labels['similarity'] = predicted_labels.apply(lambda x: get_similarity(x, parameter_groups, reference_groups), axis=1)\n",
    "\n",
    "print(f\"Filtering parameters on numeric criteria results in {test_data_num.shape[0]} parameters of which {(test_data_num.concept_label != 'unmapped').sum() / test_data_num.shape[0]:.2%} was labeled as relevant. A similarity was calculated for {predicted_labels.similarity.notna().sum()} out of {predicted_labels.shape[0]} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = './output/base/overlapping/skew/'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "\n",
    "predicted_labels['skewed_diff'] = predicted_labels.apply(lambda x: abs(parameter_groups.get(x['id'], {}).get('skewed', np.nan) - reference_groups.get(x['label'], {}).get('skewed', np.nan)), axis=1)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels.skewed_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = './output/base/overlapping/skew/keep_non_numeric_concepts/'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "\n",
    "skew_threshold = 100\n",
    "skew_results = dict()\n",
    "skew_results_counts = dict()\n",
    "skew_results_base = dict()\n",
    "\n",
    "for j in range(0, skew_threshold, 1):\n",
    "    i = j / skew_threshold\n",
    "    predicted_labels['rank_skew'] = predicted_labels.groupby(['id', ((predicted_labels['skewed_diff'] < i) | predicted_labels['skewed_diff'].isna())]).cumcount() + 1\n",
    "    predicted_labels['rank_skew'] = predicted_labels['rank_skew'].where((predicted_labels['skewed_diff'] < i) | predicted_labels['skewed_diff'].isna(), np.nan)\n",
    "\n",
    "    affected_parameters = predicted_labels.loc[(predicted_labels['rank'] == 1) & (predicted_labels['rank_skew'] != 1)]['id'].unique()\n",
    "    if len(affected_parameters) <= 1:\n",
    "        break\n",
    "    pl = predicted_labels.loc[predicted_labels['rank_skew'] == 1]\n",
    "    skew_results[i] = predicted_labels.loc[predicted_labels['id'].isin(affected_parameters) & (predicted_labels['rank_skew'] == 1)]['correct'].mean()\n",
    "    skew_results_base[i] = predicted_labels.loc[predicted_labels['id'].isin(affected_parameters)].drop_duplicates(subset=['id'])['correct'].mean()\n",
    "    skew_results_counts[i] = len(affected_parameters)\n",
    "\n",
    "plot_similarity(score_values=skew_results,\n",
    "                count_values=skew_results_counts,\n",
    "                base_values=skew_results_base,\n",
    "                save_loc=save_loc,\n",
    "                xrange=(0, 1),\n",
    "                yrange=(0, 3001),\n",
    "                analysis_title=\"Skewness\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = './output/base/overlapping/skew/drop_non_numeric_concepts/'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "\n",
    "skew_threshold = 100\n",
    "skew_results = dict()\n",
    "skew_results_counts = dict()\n",
    "skew_results_base = dict()\n",
    "\n",
    "for j in range(0, skew_threshold, 1):\n",
    "    i = j / skew_threshold\n",
    "    predicted_labels['rank_skew'] = predicted_labels.groupby(['id', ((predicted_labels['skewed_diff'] < i) | (predicted_labels['skewed_diff'].isna() & (predicted_labels['label'] == 'unmapped')))]).cumcount() + 1\n",
    "    predicted_labels['rank_skew'] = predicted_labels['rank_skew'].where((predicted_labels['skewed_diff'] < i) | (predicted_labels['skewed_diff'].isna() & (predicted_labels['label'] == 'unmapped')), np.nan)\n",
    "\n",
    "    affected_parameters = predicted_labels.loc[(predicted_labels['rank'] == 1) & (predicted_labels['rank_skew'] != 1)]['id'].unique()\n",
    "    if len(affected_parameters) <= 1:\n",
    "        break\n",
    "    pl = predicted_labels.loc[predicted_labels['rank_skew'] == 1]\n",
    "    skew_results[i] = predicted_labels.loc[predicted_labels['id'].isin(affected_parameters) & (predicted_labels['rank_skew'] == 1)]['correct'].mean()\n",
    "    skew_results_base[i] = predicted_labels.loc[predicted_labels['id'].isin(affected_parameters)].drop_duplicates(subset=['id'])['correct'].mean()\n",
    "    skew_results_counts[i] = len(affected_parameters)\n",
    "\n",
    "plot_similarity(score_values=skew_results,\n",
    "                count_values=skew_results_counts,\n",
    "                base_values=skew_results_base,\n",
    "                save_loc=save_loc,\n",
    "                xrange=(0, 1),\n",
    "                yrange=(0, 3001),\n",
    "                analysis_title=\"Skewness\",\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
